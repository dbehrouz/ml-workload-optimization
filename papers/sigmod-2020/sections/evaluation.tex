\section{Evaluation} \label{sec-evaluation} 
In this section, we evaluate the performance of our collaborative optimizer.
We focus on investigating the effect of our materialization and reuse algorithms on the execution cost of workloads in collaborative environments.
We first describe the setup, i.e., the hardware specification and the experiment workloads.
Then, we show the run-time improvement of our optimizer.
Finally, we investigate the effect of the individual contributions, i.e., materialization and reuse algorithms, on the run-time and storage cost.
\begin{table*}[t]
\begin{tabular}{lp{0.84\textwidth}rr}
\hline
\textbf{$ID$} & \textbf{$Description$}& \textbf{$N$}& \textbf{$S$}   \\
\hline
1 &  \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'start-here-a-gentle-introduction'. It includes several feature engineering operations before training logistic regression, random forest, and gradient boosted tree models.} & 397 & 14.5\\[0.4cm]

2 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'introduction-to-manual-feature-engineering'. It joins multiple datasets, preprocesses the datasets to generate features, and trains gradient boosted tree models on the generated features.} & 406 & 25\\[0.4cm]

3 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'introduction-to-manual-feature-engineering-p2'. It is similar to Workload 2, with the resulting preprocessed datasets having more features.} & 146 & 83.5\\[0.15cm]

4 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version of Workload 1 submitted by the Kaggle user "crldata". It trains a gradient boosted tree with a different set of hyperparameters.} & 280 & 10\\[0.4cm]

5 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version of Workload 1 submitted by the Kaggle user "taozhongxiao". It performs random and grid search for gradient boosted tree model using generated features of Workload 1.} & 402 & 13.8\\[0.4cm]

6 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script based on Workloads 2 and 4. It trains a gradient boosted tree on the generated features of Workload 2.} & 121 & 21\\[0.15cm]

7 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script based on Workload 3 and 4. It trains a gradient boosted tree on the generated features of Workload 3.} & 145 & 83\\[0.15cm]

8 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script that joins the features of Workloads 1 and 2. Then, similar to Workload 4, it trains a gradient boosted tree on the joined dataset.} & 341 & 21.1\\
\hline
\end{tabular}
\caption{Description of Kaggle workloads. $N$ is number of the artifacts and $S$ is the total size of the artifacts in GB.}
\label{kaggle-workload}
\end{table*}

\subsection{Setup}
We execute all the experiments on a server running Linux Ubuntu with 128 GB of RAM.
We implement a prototype of our system in python 2.7.12.
In the prototype, we implement EG using python's NetworkX 2.2 \cite{hagberg2008exploring}.
Unless specified otherwise, we run every experiment 3 times and report the average results.
We evaluate our proposed optimizations on two different sets of workloads\footnote{workload and experiment scripts will be made available in the camera ready version}.
\todo[inline]{What's the best way of letting the reviewers know we will make the code available? check footnote 3}

\textbf{Kaggle workloads.} 
In the first set of workloads, we recreate the collaborative environment of the use case in Section \ref{sec-background}.
We use a total of 8 workloads.
We introduced three workloads in the use case.
We retrieve two more workloads from the same Kaggle competition.
We also design three workloads based on the existing ones.
Table \ref{kaggle-workload} shows details of the workloads we utilize in our experiments.
The use case has nine datasets with a total size of 2.5 GB\footnote{https://www.kaggle.com/c/home-credit-default-risk/data}.
The combined size of the artifacts after executing all the workloads is around 130 GB.
Unless specified otherwise, we set the materialization budget to 16 GB (approximately 10\% of the combined size of the artifacts) and $\alpha=0.5$.
For all the experiments, we execute the workloads in order (from 1 to 8) and report the results.
We use the Kaggle workloads to show the effect of end-to-end optimization, storage-aware and heuristics-based materializations, and our reuse algorithm on executing workloads in collaborative environments.

\textbf{OpenML workloads.} In the OpenML workloads, we retrieve all the scikit-learn pipelines for task 31\footnote{https://www.openml.org/t/31}, i.e., classifying customers as good or bad credit risks using the German Credit data from the UCI repository \cite{asuncion2007uci}.
OpenML refers to ML pipelines as flows and the hyperparameter configurations as setups.
The execution of a flow with a specific setup on a dataset is called a run.
We extracted the first 2000 runs for task 31, which results in 16 unique flows. 
The dataset is small, and the total size of the artifacts after executing the 2000 runs is 1.5 GB.
For all the experiments, we execute the 2000 runs in order and report the result.
We use the OpenML workloads to show the effects of the quality ratio (i.e., $\alpha$ in the utility function for artifact materialization) and model warmstarting on workload execution in collaborative environments.
Unless specified otherwise, we set the materialization budget to 100 MB and $\alpha=0.5$.

\subsection{End-to-end Optimization}
In this experiment, we evaluate the impact of our collaborative optimizer on the use case of Section \ref{sec-background}.
In the use case, we describe three workloads (Workloads 1-3 of Table \ref{kaggle-workload}).
Kaggle reports users have copied and modified these workloads a total of 7000 times.
Therefore, at the very least, users execute these workloads 7000 times.
Figures \ref{exp-reuse-kaggle-same-workload}(a)-(c) show the result of repeating the execution of each workload twice.
Before the first run, EG is empty; therefore, both the baseline (KG) and our collaborative optimizer (CO) have similar run-times.
\todo[inline]{I need to investigate why we are better for run 1 of Workload 2 and 3 but worse for run 1 of Workload 1, (I know why we are better in 2 and 3, once I figure out the behavior of workload 1 I will update the text).}
By materializing and reusing the artifacts, CO reduces the run-time of the consecutive runs by one order of magnitude for workloads 2 and 3.
Workload 1 executes an external visualization command that computes a bivariate kernel density estimate, which incurs a large overhead.
Since our collaborative optimizer does not materialize such external information, it must re-execute the visualization command; thus, resulting in a smaller run-time reduction.
\begin{figure}
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction.pgf}%
}
\caption{Workload 1}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering.pgf}%
}
\caption{Workload 2}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering_p2.pgf}%
}
\caption{Workload 3}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads.pgf}%
}
\caption{Execution of Kaggle workloads in sequence}
\end{subfigure}
\caption{Execution of Kaggle workloads (materialization budget = 16 GB). KG = Kaggle's infrastructure (our baseline), CO = Collaborative Optimizer.}
\label{exp-reuse-kaggle-same-workload}
\end{figure}

Figure \ref{exp-reuse-kaggle-same-workload}(d) shows the cumulative run-time of executing the workloads of Table \ref{kaggle-workload} consecutively.
%This corresponds to a real scenario, where after some scripts in a collaborative environment gain popularity, i.e., Workloads 1-3 (after 7000 , other users modify and improve the scripts, i.e., Workloads 4-8.
Workloads 4-8 operate on the artifacts generated in Workloads 1-3; thus, instead of recomputing those artifacts, our collaborative optimizer reuses the existing artifacts.
As a result, the cumulative run-time of running the 8 workloads decreases by more than 50\%.
The experiment shows that optimizing even a single execution of each workload has an impact on the total run-time.
In a real collaborative environment, there are hundreds of more modified scripts and possibly thousands of repeated execution of such scripts, resulting in 1000s of hours of reduction in the cumulative run-time.

\subsection{Materialization}
In this set of experiments, we investigate the impact of our materialization algorithms on storage and run-time.

\textbf{Effect of Materialization on Storage.}
Typically, in a real collaborative environment, deciding on a reasonable materialization budget requires knowledge of the expected size of the artifacts, number of the users in a collaborative environment, and rate of incoming workloads.
In this experiment, we show that even with a small budget, our materialization algorithms, particularly our storage-aware algorithm, store a large portion of the artifacts that reappear in future workloads.
We hypothesize that there is considerable overlap between columns of different dataset artifacts in ML workloads.
Therefore, the actual total size of the artifacts our storage-aware algorithm materializes is larger than the specified budget.
We run the workloads of Table \ref{kaggle-workload} under different materialization budgets using both our heuristics-based and storage-aware materialization algorithms.
Figures \ref{exp-sa-vs-simple-size}(a)-(d) show the real size of the stored artifacts under different materialization budgets for the heuristics-based (HM) and storage-aware (SA) algorithms.
Furthermore, to show the total size of the materialized artifacts, we also implement an all materializer (represented by ALL in the figure).
The all-materializer materializes every artifact in EG.
In HM, the maximum real size is always equal to the budget.
However, in SA, we observe that the real size of the stored artifacts reaches up to 8 times the budget.
With a materialization budget of 8 GB and 16 GB, SA materializes nearly 50\% and 80\% of all the artifacts.
When the materialization budget is more than 16 GB, SA materializes nearly all the artifacts.
This indicates that there is considerable overlap between the artifacts of ML workloads.
By deduplicating the artifacts, our storage-aware materialization can materialize more artifacts.
Note that when an artifact with a high utility value has no overlap with other artifacts, SA still prioritizes it over other artifacts.
As a result, it is likely that when materializing an artifact that has no overlap with other artifacts, the total size of the materialized data decreases.
Figure \ref{exp-sa-vs-simple-size}(a) shows such an example.
After executing Workload 2, SA materializes several artifacts that overlap with each other.
However, in Workload 3, SA materializes a new artifact with a high utility, which represents a large dataset with many features (i.e., 1133 columns and around 3 GB).
Since the new artifact is large, SA removes many of the existing artifacts.
As a result, the total size of the materialized artifacts decreases after executing Workload 3.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-8.pgf}%
}
\caption{Budget = 8 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-16.pgf}%
}
\caption{Budget = 16 GB}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-32.pgf}%
}

\caption{Budget = 32 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-64.pgf}%
}
\caption{Budget = 64 GB}
\end{subfigure}
\caption{Real size of the stored artifact for storage-aware and heuristics-based materializations under different budgets (SA = storage-aware, HM = heuristics-based)}
\label{exp-sa-vs-simple-size}
\end{figure}

\textbf{Effect of Materialization on Run-time.}
Figure \ref{run-time-vs-mat} shows the total run-time of all the workloads under different budgets for heuristics-based and storage-aware algorithms.
We also plot the total run-time for the all-materializer.
Even with a materialization budget of 8 GB, SA performs similar to the scenario where all the artifacts are materialized.
On the contrary, under a small materialization budget ($\leq 16$), the HM performs 50\% worse than the other scenarios.
This is mainly because many of the preprocessed dataset artifacts are large, e.g., in Workload 3, some artifacts are more than 5 GB.
Most of these artifacts differ with each other in only a few columns.
However, the HM is unable to exploit this similarity and chooses not to materialize any of the large artifacts.
Recomputing these artifacts is costly, which results in a larger total run-time for HM under a smaller budget.

\begin{figure}
\centering
 \resizebox{0.7\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/run-time.pgf}%
}
\caption{Total run time of the workloads with different materialization algorithms and budgets}
\label{run-time-vs-mat}
\end{figure}

\textbf{Model Materialization. }
One goal of our materialization algorithms is to materialize high-quality models as soon as they appear in a new workload.
In this experiment, we run a model-benchmarking scenario, where users compare the score of their models with the score of the best performing model in the collaborative environment.
The model-benchmarking scenario represents a common real-world use-case where every user is trying to improve the best current model.
We use the OpenML workloads for the model-benchmarking scenario.
The implementation of the scenario is as follows.
First, We execute every OpenML workload one by one and keep track of the best performing workload.
Then, for every workload, as a post-processing step, we compare the score of the model with the score of the best performing model so far.
We compare the cumulative run-time of running the model-benchmarking scenario using our collaborative optimizer (CO) and the OpenML baseline (OML).
We also investigate the effect of different values of $\alpha$, the parameter controlling the importance of model potential in our materialization algorithm, on run-time.
Figure \ref{exp-model-materialization}(a) shows the cumulative run-time of executing the model-benchmarking scenario for CO and OML.
In OML, for every workload, we have to re-run the pipeline of the best performing model.
As a result, OML has a larger cumulative run-time when compared with CO.
Figure \ref{exp-model-materialization}(b) explains the reason for such a large difference between OML and CO.
When CO encounters a model that performs better than all the existing models, the materialization algorithm immediately materializes the pipeline and the model.
As a result, we reuse the model from EG instead of re-running all the operations to train the model.
In Figure \ref{exp-model-materialization}(b), we observe that reusing the best pipeline for all the 2000 OpenML workloads has an overhead of 65 seconds.
In comparison, re-executing the best pipeline for OML has an overhead of 2000 seconds.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/cumulative-runtime.pgf}%
}
\caption{Combined run-time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/bestpipeline-overhead.pgf}%
}
\caption{Best pipeline run-time}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/alpha_impact/alpha-runtime.pgf}%
}
\caption{Effect of $\alpha$ on run-time}
\end{subfigure}
\caption{Impact of model materialization in OpenML workloads (CO = Collaborative Optimizer, OML = OpenML Default)}
\label{exp-model-materialization}
\end{figure}
The parameter $\alpha$ indicates the importance of model quality when computing the utility value of an artifact.
If $\alpha$ is close to 1, then the materializer aggressively stores high-quality models and ignores their recreation time and size.
If $\alpha$ is close to 0, then the materializer prioritizes the recreation time and size over quality.
When the materialization budget is large, it is difficult to show the effect of $\alpha$.
For example, the materialization budget for the OpenML workloads is 100 MB.
Furthermore, the size of the OpenML models is typically less then 100 KB; thus, regardless of the $\alpha$ value, the materializer stores many of the models with a higher than average quality.
Therefore, we re-run the model-benchmarking scenario, but instead of using our heuristics-based or storage-aware materializer, we design a materializer that only stores the model with the highest utility value.
We run the OpenML workloads one by one and compare them with the best model so far.
We vary the value of $\alpha$ from 0 to 1.
When $\alpha$ is 1, the materializer always selects the best model to materialize, since it only considers the quality.
Therefore, $\alpha=1$ incurs the smallest cumulative run-time in the model-benchmarking scenario.
For other values of $\alpha$, we report the difference in cumulative run-time when compared to $\alpha=1$ in Figure \ref{exp-model-materialization}(c).
In the scenario, we repeatedly execute the pipeline with the best model; thus, the faster we materialize the best model, the smaller the cumulative run-time would be.
Once we materialize a model, the delta in cumulative run-time reaches a plateau.
This is because the overhead of re-executing the best model is negligible; thus, cumulative run-time becomes similar to when $\alpha=1$.
In the model-benchmarking scenario, the first time we encounter a new model with higher quality than the previous workloads is at workload number 14.
However, smaller $\alpha$ values ($\alpha<0.5$) only materialize this model after more than 100 repeated execution.
As a result, their delta in cumulative run-time reaches a plateau later than when $\alpha$ is larger.
The long delay in the materialization of the best model contributes to the higher cumulative run-time for smaller values of $\alpha$.

In our collaborative optimizer, the default value of $\alpha$ is 0.5. 
$\alpha =0.5$ provides a good balance between workloads that have the goal of training high-quality models (e.g., the model-benchmarking scenario) and workloads that are more exploratory in nature.
However, when we have prior knowledge of the nature of the workload in a collaborative environment, then we can set $\alpha$ accordingly.
Therefore, we recommend $\alpha>0.5$ for workloads with the goal of training high-quality models and $\alpha<0.5$ for workloads with exploratory data analysis.

\subsection{Reuse}
In this experiment, we evaluate the performance of our reuse algorithm.
We compare our linear time reuse algorithm (LN) with two baselines.
In the first baseline (ALL\_M), we reuse every materialized artifact.
In the second baseline (ALL\_C), we recompute every materialized artifact.
Figure \ref{reuse-experiment} shows the total run-time of the Kaggle workloads with different reuse approaches under different materialization algorithms.
The performance of ALL\_C is similar to when there is no Experiment Graph.
Thus, ALL\_C, independent of the materialization algorithm and budget, finishes the execution of all the Kaggle workloads in around 2000.
Since storage-aware materializes more artifacts than heuristics-based, ALL\_M and LN incur lower cumulative-run times in storage-aware.
For both materialization algorithms, ALL\_M has a similar performance to LN until Workload 7.
Many of the artifacts of Workload 7 incur larger load costs than compute costs.
As a result, our reuse algorithm chooses to recompute these artifacts; thus, resulting in a smaller cumulative run-time when compared with ALL\_M.
In this experiment, EG is inside the memory of the machine; thus, load times are generally low.
In scenarios where EG is on disk, we expect LN to outperform ALL\_M with a larger margin due to the very large load cost of some of the materialized artifacts.
\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/simple-mat.pgf}%
}
\caption{Heuristics-based}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/storage-aware-mat.pgf}%
}

\caption{Storage-aware}
\end{subfigure}
\caption{Comparison of our linear reuse algorithm with two baselines under different materialization schemes (LN = linear, ALL\_C = all compute, ALL\_M = all materialized).}
\label{reuse-experiment}
\end{figure}

\subsection{Warmstarting}
In this experiment, we evaluate the effect of our warmstarting method.
We execute all the OpenML workloads with and without warmstarting.
Figure \ref{exp-model-warmstarting} shows the effect of warmstarting on the OpenML workloads.
In Figure \ref{exp-model-warmstarting}(a), we observe that without warmstarting, the cumulative run-time of the baseline (OML) and our optimizer (CO-W) is nearly identical.
Warmstarting (CO+W) reduces the cumulative run-time by a third.
In OpenML workloads, because of the small size of the datasets, the run-time of the data transformation operations is very small, i.e., typically only a few milliseconds.
The model training operations are the main contributors to the total run-time.
Another characteristic of the OpenML workloads is that nearly every model training operation has a unique set of hyperparameters.
For example, out of the 2000 workloads that we execute, only 4 have similar hyperparameters.
Thus, the result of the model training operations cannot be reused, which results in CO-W having the same cumulative run-time as OML.
To further show that warmstarting is the main reason behind the smaller cumulative run-time of CO+W, we also plot the total number of executed operations in Figure \ref{exp-model-warmstarting}(b).
There are, on average, 25 operations in every OpenML workload.
OML does not reuse any artifacts; thus, it has to execute every operation.
In CO-W or CO+W, the average number of executed operations per workload is 7, i.e., 4 times smaller than the number of operations in OML.
Since the reuse procedure in CO-W and CO+W is identical, the cumulative number of executed operations is also identical.
However, due to the small run-time of preprocessing operations in OpenML workflow, we only observe an improvement in run-time with warmstarting.  



\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/warmstarting/warmstarting.pgf}%
}
\caption{Run Time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/operations_count/operations_count.pgf}%
}
\caption{Num of Operations}
\end{subfigure}
\caption{Effect of warmstarting on total run time}
\label{exp-model-warmstarting}
\end{figure}
