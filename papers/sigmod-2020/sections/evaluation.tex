\section{Evaluation} \label{sec-evaluation} 
In this section, we evaluate the performance of our collaborative optimizer.
We focus on investigating the effect of our materialization and reuse algorithms on the execution cost of workloads in collaborative environments.
We first describe the setup, i.e., the hardware specification and the experiment workloads.
Then, we show the run-time improvement of our optimizer.
Finally, we investigate the effect of the individual contributions, i.e., materialization and reuse algorithms, on the run-time and storage cost.
\begin{table*}[h]
\begin{tabular}{lp{0.84\textwidth}rr}
\hline
\textbf{$ID$} & \textbf{$Description$}& \textbf{$N$}& \textbf{$S$}   \\
\hline
1 &  \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'start-here-a-gentle-introduction'. It includes several feature engineering operations before training logistic regression, random forest, and gradient boosted tree models.} & 397 & 14.5\\[0.4cm]

2 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'introduction-to-manual-feature-engineering'. It joins multiple datasets, preprocesses the datasets to generate features, and trains gradient boosted tree models on the generated features.} & 406 & 25\\[0.4cm]

3 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'introduction-to-manual-feature-engineering-p2'. It is similar to Workload 2, with the resulting preprocessed datasets having more features.} & 146 & 83.5\\[0.15cm]

4 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version of Workload 1 submitted by the Kaggle user "crldata". It trains a gradient boosted tree with a different set of hyperparameters.} & 280 & 10\\[0.4cm]

5 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version of Workload 1 submitted by the Kaggle user "taozhongxiao". It performs random and grid search for gradient boosted tree model using generated features of Workload 1.} & 402 & 13.8\\[0.4cm]

6 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script based on Workloads 2 and 4. It trains a gradient boosted tree on the generated features of Workload 2.} & 121 & 21\\[0.15cm]

7 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script based on Workload 3 and 4. It trains a gradient boosted tree on the generated features of Workload 3.} & 145 & 83\\[0.15cm]

8 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script that joins the features of Workloads 1 and 2. Then, similar to Workload 4, it trains a gradient boosted tree on the joined dataset.} & 341 & 21.1\\
\hline
\end{tabular}
\caption{Description of Kaggle workloads. $N$ is number of the artifacts and $S$ is the total size of the artifacts in GB.}
\label{kaggle-workload}
\end{table*}
\subsection{Setup}
We execute all the experiments on a server running Linux Ubuntu with 128 GB of RAM.
We implement a prototype of our system in python 2.7.12.
In the prototype, we implement EG using python's NetworkX 2.2 \cite{hagberg2008exploring}.
We run every experiment 3 times and report the average results with error bars.
We evaluate our proposed optimizations on two different sets of workloads\footnote{We plan to release the entire codebase, i.e., the optimizer system and experiment workload scripts}.

\textbf{Kaggle workloads.} 
In the first set of workloads, we recreate the collaborative environment of the use case in Section \ref{sec-background}.
We use eight workloads, which generate a total of 130 GB of artifacts.
We introduced three workloads in the use case.
We retrieve two more workloads from the same Kaggle competition.
We also design three workloads based on the existing ones.
Table \ref{kaggle-workload} shows details of the workloads we utilize in our experiments.
There are 9 source datasets with a total size of 2.5 GB\footnote{https://www.kaggle.com/c/home-credit-default-risk/data}.
Unless specified otherwise, we use storage-aware materialization with a budget of 16 GB (approximately 10\% of the combined size of the artifacts) and $\alpha=0.5$.
For all the experiments, we execute the workloads in order (from 1 to 8) and report the results.
We use the Kaggle workloads to investigate the effect of end-to-end optimization, storage-aware and heuristics-based materializations, and our reuse algorithm.

\textbf{OpenML workloads.} In the OpenML workloads, we retrieve all the scikit-learn pipelines for Task 31 from the OpenML platform\footnote{https://www.openml.org/t/31}, i.e., classifying customers as good or bad credit risks using the German Credit data from the UCI repository \cite{asuncion2007uci}.
Task 31 is the most popular task with the highest number of ML pipeline runs.
For every task, OpenML stores a log of all the runs.
We extracted the first 2000 runs of Task 31 from the logs.
The dataset is small, and the total size of the artifacts after executing the 2000 runs is 1.5 GB.
For all the experiments, we execute the 2000 runs in order and report the result.
We use the OpenML workloads to show the effects of the quality ratio (i.e., $\alpha$ in the utility function for artifact materialization) and model warmstarting on workload execution in collaborative environments.
Unless specified otherwise,  we use storage-aware materialization with a budget of 100 MB and $\alpha=0.5$.
\subsection{End-to-end Optimization}
In this experiment, we evaluate the impact of our collaborative optimizer on the Kaggle workload.
In our motivating, we describe three workloads (Workloads 1-3 of Table \ref{kaggle-workload}).
Kaggle reports users have copied and modified these workloads a total of 7000 times.
Therefore, at the very least, users execute these workloads 7000 times.
Figures \ref{exp-reuse-kaggle-same-workload}(a)-(c) show the result of repeating the execution of each workload twice.
Before the first run, EG is empty; therefore, both the baseline (KG) and our collaborative optimizer (CO) must execute all the operation in the workloads.
In Workload 1, the run-time of CO is slightly larger than KG in the first run.
Workload 1 executes two alignment operations.
An alignment operation receives two datasets, removes all the columns that do not appear in both datasets, and returns the resulting two datasets.
In CO, we need to measure the precise compute-cost of every artifact.
This is not possible for operations that return multiple artifacts.
Thus, we re-implemented the alignment operation, which is less optimized than the baseline implementation.
In Workloads 2 and 3, CO outperforms KG even in the first.
Both Workloads 2 and 3 contain many redundant operations.
The local pruning step in CO identifies the redundancies and only execute such operations once.
In the second run of the workloads, CO reduces the run-time by one order of magnitude for workloads 2 and 3.
Workload 1 executes an external visualization command that computes a bivariate kernel density estimate, which incurs a large overhead.
Since our collaborative optimizer does not materialize such external information, it must re-execute the visualization command; thus, resulting in a smaller run-time reduction.
\begin{figure}[h]
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction.pgf}%
}
\caption{Workload 1}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering.pgf}%
}
\caption{Workload 2}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering_p2.pgf}%
}
\caption{Workload 3}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads.pgf}%
}
\caption{Execution of Kaggle workloads in sequence}
\end{subfigure}
\caption{Run-time of Kaggle workloads. KG = Default Kaggle, CO = Collaborative Optimizer.}
\label{exp-reuse-kaggle-same-workload}
\end{figure}

Figure \ref{exp-reuse-kaggle-same-workload}(d) shows the cumulative run-time of executing the workloads of Table \ref{kaggle-workload} consecutively.
%This corresponds to a real scenario, where after some scripts in a collaborative environment gain popularity, i.e., Workloads 1-3 (after 7000 , other users modify and improve the scripts, i.e., Workloads 4-8.
Workloads 4-8 operate on the artifacts generated in Workloads 1-3; thus, instead of recomputing those artifacts, our collaborative optimizer reuses the existing artifacts.
As a result, the cumulative run-time of running the 8 workloads decreases by 50\%.
The experiment shows that optimizing even a single execution of each workload has an impact on the total run-time.
In a real collaborative environment, there are hundreds of more modified scripts and possibly thousands of repeated execution of such scripts, resulting in 1000s of hours of reduction in the cumulative run-time.
\subsection{Materialization}
In this set of experiments, we investigate the impact of our materialization algorithms on storage and run-time.

\begin{figure}[t]
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-8.pgf}%
}
\caption{Budget = 8 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-16.pgf}%
}
\caption{Budget = 16 GB}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-32.pgf}%
}
\caption{Budget = 32 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-64.pgf}%
}
\caption{Budget = 64 GB}
\end{subfigure}
\caption{Real size of the stored artifact for storage-aware (SA) and heuristics-based (HM) materializations}
\label{exp-sa-vs-simple-size}
\end{figure}
\textbf{Effect of Materialization on Storage.}
Typically, in a real collaborative environment, deciding on a reasonable materialization budget requires knowledge of the expected size of the artifacts, number of the users in a collaborative environment, and rate of incoming workloads.
The goal of this experiment is to show that even with a small budget, our materialization algorithms, particularly our storage-aware algorithm, store a large portion of the artifacts that reappear in future workloads.
We hypothesize that there is considerable overlap between columns of different dataset artifacts in ML workloads.
Therefore, the actual total size of the artifacts our storage-aware algorithm materializes is larger than the specified budget.
We run the workloads of Table \ref{kaggle-workload} under different materialization budgets using both our heuristics-based and storage-aware materialization algorithms.
Figures \ref{exp-sa-vs-simple-size}(a)-(d) show the real size of the stored artifacts under different materialization budgets for the heuristics-based (HM) and storage-aware (SA) algorithms.
Furthermore, to show the total size of the materialized artifacts, we also implement an all materializer (represented by ALL in the figure).
The all-materializer materializes every artifact in EG.
In HM, the maximum real size is always equal to the budget.
However, in SA, we observe that the real size of the stored artifacts reaches up to 8 times the budget.
With a materialization budget of 8 GB and 16 GB, SA materializes nearly 50\% and 80\% of all the artifacts.
When the materialization budget is more than 16 GB, SA materializes nearly all the artifacts.
This indicates that there is considerable overlap between the artifacts of ML workloads.
By deduplicating the artifacts, our storage-aware materialization can materialize more artifacts.
Note that when an artifact with a high utility value has no overlap with other artifacts, SA still prioritizes it over other artifacts.
As a result, it is likely that when materializing an artifact that has no overlap with other artifacts, the total size of the materialized data decreases.
Figure \ref{exp-sa-vs-simple-size}(a) shows such an example.
After executing Workload 2, SA materializes several artifacts that overlap with each other.
However, in Workload 3, SA materializes a new artifact with a high utility, which represents a large dataset with many features (i.e., 1133 columns and around 3 GB).
Since the new artifact is large, SA removes many of the existing artifacts.
As a result, the total size of the materialized artifacts decreases after executing Workload 3.

\textbf{Effect of Materialization on Run-time.}
The goal of this experiment is to show that SA has a small run-time with a low materialization budget.
Figure \ref{run-time-vs-mat} shows the total run-time of all the workloads under different budgets for heuristics-based and storage-aware algorithms.
We also plot the total run-time for the all-materializer.
Even with a materialization budget of 8 GB, SA has a comparable performance to the scenario where all the artifacts are materialized (i.e., difference in run-time is 100 seconds).
On the contrary, under a small materialization budget ($\leq 16$), HM performs on average 50\% worse than SA.
For larger materialization budgets ($\geq 16$), HM performs better, however, its performance is still around 40\% worse than SA.
This is mainly because many of the preprocessed dataset artifacts are large, e.g., in Workload 3, some artifacts are more than 3 GB.
Most of these artifacts differ with each other in only a few columns.
However, HM is unable to exploit this similarity and chooses not to materialize any of the large artifacts.
Recomputing these artifacts is costly, which results in a larger total run-time for HM under a smaller budget.
\begin{figure}[t]
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/run-time.pgf}%
}
\caption{Total run-time of the workloads with different materialization algorithms and budgets}
\label{run-time-vs-mat}
\end{figure}

\textbf{Model Materialization. }
One goal of our materialization algorithms is to materialize high-quality models as soon as they appear in a new workload.
In this experiment, we run a model-benchmarking scenario, where users compare the score of their models with the score of the best performing model in the collaborative environment.
The model-benchmarking scenario represents a common real-world use-case where every user is trying to improve the best current model.
We use the OpenML workloads for the model-benchmarking scenario.
The implementation of the scenario is as follows.
First, We execute every OpenML workload one by one and keep track of the best performing workload.
Then, for every workload, as a post-processing step, we compare the score of the model with the score of the best performing model so far.
We compare the cumulative run-time of running the model-benchmarking scenario using our collaborative optimizer (CO) with default configuration (i.e., storage-aware materializer with budget 100 MB and $\alpha=0.5$) and the OpenML baseline (OML).
Figure \ref{exp-model-materialization}(a) shows the cumulative run-time of executing the model-benchmarking scenario for CO and OML (i.e., the combined run-time of the current and the best workload).
In OML, for every workload, we have to re-run the pipeline of the best performing model.
As a result, OML has a larger cumulative run-time when compared with CO.
Figure \ref{exp-model-materialization}(b) explains the reason for such a large difference between OML and CO.
When CO encounters a model that performs better than all the existing models, the materialization algorithm immediately materializes the pipeline and the model.
As a result, we reuse the model from EG instead of re-running all the operations to train the model.
In Figure \ref{exp-model-materialization}(b), we observe that reusing the best pipeline for all the 2000 OpenML workloads has an overhead of 65 seconds.
In comparison, re-executing the best pipeline for OML has an overhead of 2000 seconds.

\begin{figure}[t]
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/cumulative-runtime.pgf}%
}
\caption{Combined run-time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/bestpipeline-overhead.pgf}%
}
\caption{Best pipeline run-time}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/alpha_impact/alpha-runtime.pgf}%
}
\caption{Difference in run-time when compared to $\alpha=1$}
\end{subfigure}
\caption{Impact of model materialization and $\alpha$ on executing the model-benchmarking scenario (OML = OpenML (baseline), CO = Collaborative Optimizer)}
\label{exp-model-materialization}
\end{figure}
We also investigate the effect of different values of $\alpha$, the parameter controlling the importance of model potential in our materialization algorithm, on run-time.
If $\alpha$ is close to 1, then the materializer aggressively stores high-quality models and ignores their recreation time and size.
If $\alpha$ is close to 0, then the materializer prioritizes the recreation time and size over quality.
When the materialization budget is large, it is difficult to show the effect of $\alpha$.
For example, the materialization budget for the OpenML workloads is 100 MB.
Furthermore, the size of the OpenML models is typically less than 100 KB; thus, regardless of the $\alpha$ value, the materializer stores many of the models with a higher than average quality.
Therefore, in this experiment, we design a materializer that only materializes one model artifact from the entire EG.
This highlights the impact of $\alpha$ on the utility function.
We run the OpenML workloads one by one and compare them with the best model so far.
We vary the value of $\alpha$ from 0 to 1.
When $\alpha$ is 1, the materializer always selects the best model to materialize, since it only considers the quality.
Therefore, $\alpha=1$ incurs the smallest cumulative run-time in the model-benchmarking scenario.
In Figure \ref{exp-model-materialization}(c), we report the difference in cumulative run-time between $\alpha=1$ and other values of $\alpha$ (i.e., the line y=0 corresponds to the delta in cumulative run-time when $\alpha=1$).
Note that because of the small y-scale, the variation of the reported result (i.e., the error band) is more visible when compared to the previous experiments.
In the scenario, we repeatedly execute the pipeline with the best model; thus, the faster we materialize the best model, the smaller the cumulative run-time would be.
Once we materialize a model, the delta in cumulative run-time reaches a plateau.
This is because the overhead of re-executing the best model is negligible; thus, cumulative run-time becomes similar to when $\alpha=1$.
The first time we encounter a new model with higher quality than the previous workloads is at workload number 14.
However, smaller $\alpha$ values ($\alpha<0.5$) only materialize this model after more than 100 repeated execution.
As a result, their delta in cumulative run-time reaches a plateau later than large $\alpha$ values.
The long delay in the materialization of the best model contributes to the higher cumulative run-time for smaller values of $\alpha$.

In our collaborative optimizer, the default value of $\alpha$ is 0.5. 
This value provides a good balance between workloads that have the goal of training high-quality models (e.g., the model-benchmarking scenario) and workloads that are more exploratory in nature.
However, when we have prior knowledge of the nature of the workload in a collaborative environment, then we can set $\alpha$ accordingly.
Therefore, we recommend $\alpha>0.5$ for workloads with the goal of training high-quality models and $\alpha<0.5$ for workloads with exploratory data analysis.
\begin{figure}[h]
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/simple-mat.pgf}%
}
\caption{Heuristics-based}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/storage-aware-mat.pgf}%
}
\caption{Storage-aware}
\end{subfigure}
\caption{Run-time of linear-time (LN), all compute (ALL\_C), and all materialized (ALL\_M) reuse methods.}
\label{reuse-experiment}
\vspace{-8mm}
\end{figure}
\subsection{Reuse}
In this experiment, we evaluate the performance of our reuse algorithm on Kaggle workloads.
We compare our linear time reuse algorithm (LN) with two baselines.
In the first baseline (ALL\_M), we reuse every materialized artifact.
In the second baseline (ALL\_C), we recompute every artifact.
Figure \ref{reuse-experiment} shows the total run-time of the Kaggle workloads with different reuse approaches under different materialization algorithms.
ALL\_C, independent of the materialization algorithm and budget, finishes the execution of all the Kaggle workloads in around 2000.
For HM materialization, all three reuse algorithms have similar performance until Workload 6.
Since Workload 3 has large artifacts, HM exhausts its budget by materializing them (as explained the materialization experiments).
Furthermore, Workloads 4, 5, and 6 are modified versions of Workloads 1 and 2 (Table \ref{kaggle-workload}).
Therefore, there are not many reuse opportunities until Workload 7.
However, for SA materialization, both ALL\_M and LN outperform ALL\_C starting from Workload 4.
SA has better utilization of the budget and materializes some of the artifacts of the Workload 1 and 2.
ALL\_M and LN reuse these artifacts in Workloads 4, 5, and 6; thus, incurring a smaller run-time.
For both materialization algorithms, ALL\_M has a similar performance to LN until Workload 7.
Many of the artifacts of Workload 7 incur larger load costs than compute costs.
As a result, LN recomputes these artifacts and results in a smaller cumulative run-time than ALL\_M, i.e., around 300 seconds in both HM and SA materialization.
In this experiment, EG is inside the memory of the machine; thus, load times are generally low.
In scenarios where EG is on disk, we expect LN to outperform ALL\_M with a larger margin due to the higher load cost of the materialized artifacts.
\begin{figure}[t]
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/warmstarting/warmstarting.pgf}%
}
\caption{Run Time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/operations_count/operations_count.pgf}%
}
\caption{Num of Operations}
\end{subfigure}
\caption{Impact of warmstarting on OpenML workloads}
\label{exp-model-warmstarting}
\vspace{-6mm}
\end{figure}
\subsection{Warmstarting}
In this experiment, we evaluate the effect of our warmstarting method.
We execute all the OpenML workloads with and without warmstarting.
Figure \ref{exp-model-warmstarting} shows the effect of warmstarting on the OpenML workloads.
In Figure \ref{exp-model-warmstarting}(a), we observe that without warmstarting, the cumulative run-time of the baseline (OML) and our optimizer (CO-W) is nearly identical.
Warmstarting (CO+W) reduces the total run-time by a third.
In OpenML workloads, because of the small size of the datasets, the run-time of the data transformation operations is very small, i.e., typically only a few milliseconds.
The model training operations are the main contributors to the total run-time.
Another characteristic of the OpenML workloads is that nearly every model training operation has a unique set of hyperparameters.
For example, out of the 2000 workloads that we execute, only 4 have similar hyperparameters.
Thus, the result of the model training operations cannot be reused, which results in CO-W having the same cumulative run-time as OML.
To further show that warmstarting is the main reason behind the smaller cumulative run-time of CO+W, we also plot the total number of executed operations in Figure \ref{exp-model-warmstarting}(b).
There are, on average, 25 operations in every OpenML workload.
OML does not reuse any artifacts; thus, it has to execute every operation.
In CO-W or CO+W, the average number of executed operations per workload is 7, i.e., 4 times smaller than the number of operations in OML.
Since the reuse procedure in CO-W and CO+W is identical, the cumulative number of executed operations is also identical.
However, due to the small run-time of preprocessing operations in OpenML workloads, we only observe an improvement in run-time with warmstarting.  