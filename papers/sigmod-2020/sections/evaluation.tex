\section{Evaluation} \label{sec-evaluation} 
\todo[inline]{most of the figures need a bit of post-processing to get the font size on the same scale and improve their quality}
In this section, we evaluate the performance of our collaborative optimizer system.
We focus on investigating the effect of our materialization and reuse algorithms on the execution cost of workloads in collaborative environments.
We first describe the setup, i.e., the hardware specification and the experiment workloads.
Then, we show the run-time improvement of our optimizer.
Finally, we investigate the effect of the individual contributions, i.e., materialization and reuse algorithms, on the run-time and storage cost.
\begin{table*}[t]
\begin{tabular}{lp{0.84\textwidth}rr}
\hline
\textbf{$ID$} & \textbf{$Description$}& \textbf{$N$}& \textbf{$S$}   \\
\hline
1 &  \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Real script titled 'start-here-a-gentle-introduction'. It includes several feature engineering operations before training logistic regression, random forest, and gradient boosted tree models.} & 397 & 14.5\\[0.4cm]

2 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Real script titled 'introduction-to-manual-feature-engineering'. It joins multiple tables to generate a dataset and trains gradient boosted tree models.} & 406 & 25\\[0.4cm]

3 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Real script titled 'introduction-to-manual-feature-engineering-p2'. It is similar to workload 2, but with larger datasets.} & 146 & 83.5\\[0.15cm]

4 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version workload 1 submitted by the Kaggle user: 'crldata'. It modifies the hyperparameters of the gradient boosted tree.} & 280 & 10\\[0.4cm]

5 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version workload 1 submitted by the Kaggle user: 'taozhongxiao'. It runs random and grid search for gradient boosted tree model on the dataset of workload 1.} & 402 & 13.8\\[0.4cm]

6 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Custom script based on workload 2. It trains a gradient boosted tree on the dataset of workload 2.} & 121 & 21\\[0.15cm]

7 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Custom script based on workload 3. It trains a gradient boosted tree on the dataset of workload 3.} & 145 & 83\\[0.15cm]

8 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Custom script that joins the features of workload 1 and 2. Then, it trains a gradient boosted tree on the joined dataset.} & 341 & 21.1\\
\hline
\end{tabular}
\caption{Kaggle workloads description. $N$ is number of the artifacts and $S$ is total size of the artifacts in GB.}
\label{kaggle-workload}
\end{table*}

\subsection{Setup}
We execute all the experiments on a server running Linux Ubuntu with 128 GB of RAM.
We implement a prototype of our system in python 2.7.12.
In the prototype, we implement EG using python's NetworkX 2.2 \cite{hagberg2008exploring}.
Unless specified otherwise, we run every experiment 3 times and report the average results.
We evaluate our proposed optimizations on two different sets of workloads\footnote{workload and experiment scripts will be made available in the camera ready version}.
\todo[inline]{What's the best way of letting the reviewers know we will make the code available? check footnote 3}

\textbf{Kaggle workloads.} 
In the first set of workloads, we recreate the collaborative environment of the use case in Section \ref{sec-background}.
We introduced three workloads in the use case.
We retrieve two more workloads from the same Kaggle competition.
We also design three workloads based on the existing ones.
Table \ref{kaggle-workload} shows details of the workloads we utilize in our experiments.
The use case has nine datasets with a total size of 2.5 GB\footnote{https://www.kaggle.com/c/home-credit-default-risk/data}.
The combined size of the artifacts after executing all the workloads is around 130 GB.
Unless specified otherwise, we set the materialization budget to 32 GB.
For all the experiments, we execute the workloads in order (from 1 to 8) and report the results.

\textbf{OpenML workloads.} In the OpenML workloads, we retrieve all the scikit-learn pipelines for task 31\footnote{https://www.openml.org/t/31}, i.e., classifying customers as good or bad credit risks using the German Credit data from the UCI repository \cite{asuncion2007uci}.
OpenML refers to ML pipelines as flows and the hyperparameter configurations as setups.
The execution of a flow with a specific setup on a task is called a run.
We extracted the first 2000 runs for task 31, which results in 16 unique flows. 
The dataset is small, and the total size of the artifacts after executing the 2000 runs is 1,5 GB.
For all the experiments, we execute the 2000 runs in order and report the result.

\subsection{Execution Time}
In this experiment, we evaluate the impact of our collaborative optimizer on the use case of Section \ref{sec-background}.
In the use case, we describe three workloads (Workloads 1-3 of Table \ref{kaggle-workload}).
Kaggle reports users have copied and modified these workloads a total of 7000 times.
Therefore, at the very least, users execute these workloads 7000 times.
Figures \ref{exp-reuse-kaggle-same-workload}(a)-(c) show the result of repeating the execution of each workload.
Before the first run, EG is empty; therefore, both the baseline (KG) and our collaborative optimizer (CO) have similar run-times.
By materializing and reusing the artifacts, CO reduces the run-time of the consecutive executions by one order of magnitude for workloads 2 and 3.
Workload 1 executes an external visualization command that computes a bivariate kernel density estimate, which incurs a large overhead.
Since our collaborative optimizer does not materialize such external information, it must re-execute the visualization command; thus, resulting in a smaller run-time reduction.
\begin{figure}
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction.pgf}%
}
\caption{Workload 1}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering.pgf}%
}
\caption{Workload 2}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering_p2.pgf}%
}
\caption{Workload 3}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads.pgf}%
}
\caption{Execution of Kaggle workloads in sequence}
\end{subfigure}
\caption{Execution of Kaggle workloads (materialization budget = 32 GB). KG = Kaggle's infrastructure (our baseline), CO = Collaborative Optimizer.}
\label{exp-reuse-kaggle-same-workload}
\end{figure}

Figure \ref{exp-reuse-kaggle-same-workload}(d) shows the cumulative run-time of executing the workloads of Table \ref{kaggle-workload} in sequence.
This corresponds to a real scenario, where after some scripts in a collaborative environment gain popularity, i.e., Workloads 1-3, other users modify and improve the scripts, i.e., Workloads 4-8.
The experiment shows that optimize even a single execution of each workload decreases the cumulative run-time by half.
In a real collaborative environment, there are hundreds of more modified scripts and possibly thousands of repeated execution of such scripts, resulting in 1000s of hours of reduction in the cumulative run-time.

\subsection{Materialization}
In this set of experiments, we show the impact of our materialization algorithms on the total size of the stored artifacts and total run-time of the workloads

\textbf{Effect of Materialization on Run-time.}
Typically, in a real collaborative environment, deciding on a reasonable materialization budget requires knowledge of the expected size of the artifacts, number of the users in a collaborative environment, and rate of incoming workloads.
In this experiment, we show that even with a small budget, our materialization algorithms, particularly our storage-aware algorithm, store a large portion of the artifacts that reappear in future workloads.
We run the workloads of Table \ref{kaggle-workload} under different materialization budgets using both our heuristics-based and storage-aware materialization algorithms.
Figures \ref{exp-sa-vs-simple-size}(a)-(d) show the real size of the stored artifacts under different materialization budgets for the heuristics-based (HM) and storage-aware (SA) algorithms.
Furthermore, to show the total size of the materialized artifacts, we also implement an all materializer (represented by ALL in the figure).
The all-materializer stores every artifact of the workloads in EG.
In HM, the maximum real size is always equal to the budget.
However, in SA, we observe that the real size of the stored artifacts reaches up to 8 times the budget.
With a materialization budget of 8 GB, SA materializes almost half of the artifacts.
When the materialization budget is more than 8 GB, SA materializes nearly all the artifacts.
This indicates that there is considerable overlap between the artifacts of ML workloads.
By deduplicating the artifacts, our storage-aware materialization algorithm can materialize more artifacts.
As a result, the storage-aware results in a smaller total run-time when compared with the heuristics-based materialization.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-8.pgf}%
}
\caption{Budget = 8 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-16.pgf}%
}

\caption{Budget = 16 GB}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-32.pgf}%
}

\caption{Budget = 32 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-64.pgf}%
}
\caption{Budget = 64 GB}
\end{subfigure}
\caption{Real size of the stored artifact for storage aware and simple materialization under different budgets (SA = storage-aware materialization, HM = heuristics-based materialization)  \todo[inline]{Need to properly explain the sudden rise at workload 7 for SA budget = 16}}
\label{exp-sa-vs-simple-size}
\end{figure}

Figure \ref{run-time-vs-mat} shows the total run-time of all the workloads under different budgets for heuristics-based and storage-aware algorithms.
We also plot the total run-time for the all-materializer.
Even with a materialization budget of 8 GB, SA performs similar to the scenario where all the artifacts are materialized.
On the contrary, under a small materialization budget ($\leq 16$), the HM performs 50\% worse than the other scenarios.
This is mainly because many of the preprocessed dataset artifacts are large, e.g., in Workload 3, some artifacts are more than 5 GB.
Most of these artifacts differ with each other in only a few columns.
However, the HM is unable to exploit this similarity and chooses not to materialize any of the large artifacts.
Recomputing these artifacts is costly, which results in a larger total run-time for heuristics-based materialization under a smaller budget.

\begin{figure}
\centering
 \resizebox{0.7\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/run-time.pgf}%
}
\caption{Total run time of the workloads with different materialization algorithms and budgets}
\label{run-time-vs-mat}
\end{figure}

\textbf{Model Materialization. }
One goal of our materialization algorithms is to materialize high-quality models as soon as they appear in a new workload.
In this experiment, we run a typical scenario, where users compare the score of their models with the score of the best performing model in the collaborative environment.
We execute all the OpenML workloads one by one and keep track of the best performing workload.
In every workload, as a post-processing step, we compare the score of the model with the score of the best performing model so far.
\todo[inline]{Will re-run this experiment with different $\alpha$ values (Utility function in Section \ref{subsec-ml-based-materialization}}
Figure \ref{exp-model-materialization}(a) shows the total run-time of executing all the OpenML workloads and comparing the model of each workload with the best performing model so far.
In the baseline (OML), for every workload, we have to rerun the pipeline of the best performing model.
As a result, OML has a much higher total run-time when compared with our collaborative optimizer (CO).
Figure \ref{exp-model-materialization}(b) explains the reason for such a large difference between OML and CO.
When CO encounters a model that performs better than all the existing models, the materialization algorithm immediately stores the pipeline and the model.
As a result, we reuse the model from EG instead of re-running all the operations.
In Figure \ref{exp-model-materialization}(b), we observe that reusing the best pipeline for all the 2000 OpenML workloads has an overhead of 65 seconds.
In comparison, re-executing the best pipeline for OML has an overhead of 2000 seconds.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/cumulative-runtime.pgf}%
}
\caption{Combined run-time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/bestpipeline-overhead.pgf}%
}
\caption{Best pipeline run-time}
\end{subfigure}
\caption{Effect of model materialization for OpenML workloads (CO = Collaborative Optimizer, OML = OpenML Default)}
\label{exp-model-materialization}
\end{figure}

\subsection{Reuse}
In this experiment, we evaluate the performance of our reuse algorithm.
We compare our linear time reuse algorithm (LN) with two baselines.
In the first baseline (ALL\_M), we reuse every materialized artifact.
In the second baseline (ALL\_C), we recompute every materialized artifact.
The performance of ALL\_C is similar to when there is no Experiment Graph.
Thus, ALL\_C finishes the execution of all the Kaggle workloads in around 2000.
ALL\_M has a similar performance to LN until workload 7.
In workload 7, there are multiple large artifacts which have a larger load cost than compute cost.
As a result, our reuse algorithm chooses to recompute these artifacts; thus, resulting in a smaller cumulative run-time when compared with ALL\_M.
In this experiment, EG is inside the memory of the machine; thus, load times are generally low.
In scenarios where EG is on disk, we expect LN to outperform ALL\_M with a larger margin due to the very large load cost of some of the materialized artifacts.
\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/simple-mat.pgf}%
}
\caption{Simple}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/storage-aware-mat.pgf}%
}

\caption{Storage-aware}
\end{subfigure}
\caption{Comparison of our linear reuse algorithm with two baselines under different materialization schemes (LN = linear, ALL\_C = all compute, ALL\_M = all materialized).}
\end{figure}

\subsection{Warmstarting}
\todo[inline]{Experiment is in progress}
In this experiment, we evaluate the effect of our warmstarting method.
We execute all the OpenML workloads with and without warmstarting.
Figure \ref{exp-model-warmstarting} shows the effect of warmstarting on the OpenML workload.
In Figure \ref{exp-model-warmstarting}(a) we observer that when no warmstarting is enabled, the run time of the baseline (OML) and our optimizer without warmstarting (CO-W) is nearly identical.
With warmstarting is enabled (CO+W), we are able to reduce to the cumulative run time by a third.
CO-W indeed manages to reuse many already computed intermediate artifacts.
Since in the OpenML workloads, the datasets are small, the data transformation and feature engineering operations are very fast.
The majority of time is spent in the model training operations.
Thus, by reusing artifacts we manage to reduce the number of executed operations, as shown in Figure \ref{exp-model-warmstarting}(b),  it hardly has an impact on the total run time.
Whereas, warmstarting decreases the total run time drastically.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/warmstarting/warmstarting.pgf}%
}
\caption{Run Time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/operations_count/operations_count.pgf}%
}
\caption{Num of Operations}
\end{subfigure}
\caption{Effect of warmstarting on total run time}
\label{exp-model-warmstarting}
\end{figure}

\subsection{Discussion}
