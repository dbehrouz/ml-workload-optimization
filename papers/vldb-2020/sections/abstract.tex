\begin{abstract}
Collaborative data science platforms, such as Google Colaboratory and Kaggle, affected the way users solve machine learning tasks.
Instead of solving a task in isolation, users write their machine learning workloads and execute them on these platforms and share the workloads with others.
This enables other users to learn from, modify, and make improvements to the existing workloads.
However, this collaborative approach suffers from two inefficiencies.
First, storing all the artifacts, such as raw datasets, generated features, and models with their hyperparameters, requires massive amounts of storage.
As a result, only some of the artifacts such as scripts, operations, and models are stored and users must re-execute the scripts and operations to reconstruct the desired artifact.
\hl{Second, even if all the artifacts are stored, manually finding desired artifacts is a time-consuming process.} % Tilmann: this is only a conclusion of the first problem, better say finding the artifacts is slow or similar... *Behrouz: I wanted to use the word manually (or something equal) to contrast between the current 'manual' querying or reading the scripts vs our way of reuse or warmstarting that doesn't require user's intervention.

The contributions of this paper are two-fold.
First, we utilize a graph to store the artifacts and operations of machine learning workloads as vertices and edges respectively. 
Since storing all the artifacts is not feasible, we propose two algorithms for selecting the artifacts with high expected rates of future reuse.
We then store the selected artifacts in memory for quick access, a process which we call artifact materialization.
The algorithms consider several metrics, such as access frequency, size of the artifact, and quality of machine learning model artifacts to decide what artifacts to materialize.
Second, using the graph, we propose three optimizations, namely reuse, model warmstarting, and fast hyperparameter tuning, to speed up the execution of the future workloads and increase the efficiency of collaborative data science platforms.

%Machine learning workloads have varying characteristics.
%Some involve a large user base where a combination of experts and novice users are trying to design machine learning pipelines and execute them on specific tasks, such as online education, data science challenges.
%Some involve fewer users, typically experts, working together to solve a task.
%For example, a team of data scientists in a company trying to design a recommender system based on the available training data.
%Both workloads are interactive and require many iterations to improve the solution.
%In such scenarios, communication between the users involved is not optimal and as a result, many repetitions may occur.
%Repetitions can be of the form of repeated data preprocessing, hyperparameter search, and model training.
%
%Using experiment databases, where a log of previous machine learning experiments is stored, we propose a solution that utilizes the information in the experiment database to improve the process of design and execution of machine learning workloads.
%Specifically, we utilize the logs in the experiment databases to reduce the data processing and model training time by caching and reusing the preprocessed data and trained models.
%Moreover, we leverage the logs to enhance the hyperparameter optimization process and provide the users (both expert and novice) with better hyperparameter settings in a shorter amount of time.
\end{abstract}