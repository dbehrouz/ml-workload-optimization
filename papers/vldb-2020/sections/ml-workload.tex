\section{Machine Learning Workloads} \label{sec-ml-workloads}
In this section, we first describe the common types of operations in machine learning workloads.
Then, we discuss how to capture and store the operations in the experiment database.
We also introduce a materialization strategy for only storing the generated data artifacts during the execution of a workload.
Lastly, we discuss how do we utilize the experiment database to optimize new workloads.

\subsection{Operations in ML Workloads}
We assume the main units of work are dataframe (e.g., Pandas, R Data Frames, and Spark DataFrames) like objects that contain one or many columns, where all the data items in one column are of the same data type.
We divide the operations in the ML workloads into 3 categories.
\begin{table}
\centering
\begin{tabular}{ll}
\hline
	   Feature Extraction & Feature Selection\\ \hline
        feature hasher & variance threshold  \\
        one hot encoding & select k best \\
        count vectorizer& select percentile \\ 
        tfidf transformer & recursive feature elimination \\
        hashing vectorizer & select from model \\
        extract\_patch\_2d &  \\
        \hline
\end{tabular}
\caption{List of feature extraction and feature selection operations}\label{feature-engineering-operations}
\end{table}

\subsubsection{Data and Feature engineering}
This group of operations typically belongs to three categories, i.e., simple data transformations and aggregations, feature selection, and feature extraction.
All of these operations, receive one or multiple columns of a dataset and return another dataset as result. 
While different data processing tools may provide specialized data transformation and aggregation operations for dataframe objects, most of them provide the same or similar operations such as map, reduce, group by, concatenation, and join. 
In Table \ref{feature-engineering-operations}, we show a list of the most common feature extraction and feature selection operations.

\subsubsection{Model Training}
Model training operations are a group of operations that receive a dataset (or one or multiple columns of a dataset) and return a machine learning model.
The result of model training operations can either be used in other data and feature engineering operations (e.g., applying PCA to reduce the number of dimensions of the data) or can be used to perform prediction (for classification and regression tasks) on unseen data.

\subsubsection{Hyperparameter Tuning}
Before training a machine learning model, one has to set the hyperparameters of the model to appropriate values.
Typically, the best values for the hyperparameters of a model vary across different datasets.
The goal of hyperparameter tuning operations is to find the machine learning models with the best performance.
A hyperparameter tuning operation is defined by a budget and a search method.
The budget specifies how many models with different hyperparameter values the operation should train and the search method specifies what search strategy should be incorporated.
We limit our focus to popular search methods, namely, grid search, random search, and Bayesian hyperparameter search \cite{bergstra2012random,snoek2012practical}.

\subsection{Graph Representation}\label{sub-graph-construction}
To efficiently apply our optimizations, we utilize a graph data structure (called the experiment graph) to store the meta-data and artifacts of the machine learning workloads.
Let $\mathcal{V}=\{v_i\}, i = 1, \cdots, n$ be a collection of artifacts that exist in the workload.
Each artifact is either a raw dataset, a pre-processed dataset resulting from a feature engineering operation, or a model resulting from a model training operation.
Let $\mathcal{E}=\{e_i\}, i = 1, \cdots, m$ be a collection of executed operations that exist in the workload.
A directed edge $e$ from $v_i$ to $v_j$ in $\mathcal{G}(\mathcal{V},\mathcal{E})$ indicates that the artifact $v_j$ is (fully or partially) derived from the artifact $v_i$ by applying the operation in $e$.
Every vertex $v$ has the attribute $\langle s \rangle$ (accessed by $v.s$), which represents the storage size of artifact when materialized.
Every edge $e$ has the attributes $\langle f, t\rangle$ (accessed by $e.f$ and $e.t$), where $f$ represents the frequency of the operation (the number of times the operation has been executed) and $t$ represents the average run-time (in seconds) of the operation.
Each vertex contains meta-data about the artifacts, such as the name and type of the columns for datasets and name, size, the value of parameters and hyperparameters, and the error metric of the models.
Each edge contains the meta-data about the operation, such as the function name, training algorithm, hyperparameters, and in some cases even the source code of the operation.
When a new machine learning workload is executed, we extend the graph to capture the new operations and artifacts.
If an operation already exists in the graph, we update the frequency and average run-time attributes.
Otherwise, we add a new edge and vertex to the experiment graph, representing the new operation and the artifact.
Figure \ref{fig-experiment-graph}a shows an example graph constructed from the code in Listing \ref{listing-experiment-graph}.
To uniquely identify an edge, we utilize a hash function that receives as input the operation and hyperparameters.

\begin{lstlisting}[language=Python, caption=Example script,captionpos=b,label = {listing-experiment-graph}]
import numpy as np
import pandas as pd

from sklearn import svm
from sklearn.feature_selection import SelectKBest
from sklearn.feature_extraction.text import CountVectorizer

train = pd.read_csv('../input/train.csv') 
print train.columns # [ad_desc,ts,u_id,price,y]
vectorizer = CountVectorizer()
count_vectorized = vectorizer.fit_transform(train['ad_desc'])
selector =  SelectKBest(k=2)
top_features = selector.fit_transform(train[['ts','u_id','price']], 
				      train['y'])
X = pd.concat([count_vectorized,top_features], axis = 1)
model = svm.SVC()
model.fit(X, train['y'])
\end{lstlisting}

\begin{figure}
\begin{subfigure}[b]{0.4\linewidth}
\centering
\input{../images/tikz/example-graph.tex}
\caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.6\linewidth}
\begin{tabular}{lcl}
\hline
operation & label &  hash \\
\hline
project(ad..) & $\langle 2, 2\rangle$ &p1 \\
project(ts, ..) & $\langle 1, 6\rangle$ & p2\\
project(y) & $\langle 1, 2\rangle$ & p3\\
vectorizer.f\_t & $\langle 2, 40\rangle$ & v1 \\
selector.f\_t & $\langle 1, 60\rangle$ & s1 \\
concat & $\langle 1, 10\rangle$ & c1 \\
merge & $\langle 1, 0 \rangle$ & m\\
model.fit & $\langle 1, 100\rangle$ & f\\
\hline
\end{tabular}
\caption{}
\end{subfigure}
\caption{Experiment graph constructed from the Listing \ref{listing-experiment-graph} (a) and the hash of the operations in the scripts (b)}
\label{fig-experiment-graph}
\end{figure}
Table \ref{fig-experiment-graph}b shows both the label of every edge operation, i.e., frequency and time, and the hash of the operations and their hyperparameters.
We assume the operations project(ad..) and vectorizer.f\_t already exist in the experiment graph.
Therefore, when adding the new operations, we must update their frequency to 2.
In order to represent operations that process multiple input artifacts, e.g., concat and model.fit operations in Listing \ref{listing-experiment-graph}, we proceed as follows.
First, we merge the vertices representing the artifacts into a single vertex using a merge operator.
The merge operator is a logical operator which does not incur a cost (run-time of 0 seconds).
Then, we draw an edge from the merged vertex which represents the actual operation.
For example, in Figure \ref{fig-experiment-graph}a, before applying the concatenation operation, we merge $v_4$ and $v_5$ into $v_6$, then we apply the concatenation operation (c1).
This is a critical step for our materialization algorithm in Section \ref{subsec-materialization} and our optimization strategies discussed in Sections \ref{sec-reuse-and-warmstarting} and \ref{sec-hyperparam-optimization}.

It is important to note that based our definition of a task in Section \ref{sec-introduction} we the workloads belong to multiple different tasks, the constructed graph will contain one connected component for every task.
In the next sections, we assume that the experiment graph contains information about 1 task, although, all the methods described can be applied to multiple tasks as well.

\subsection{Materialization of the Experiment Graph}\label{subsec-materialization}
Depending on the number of the executed workloads, the generated artifacts may require a large amount of storage space.
For example, in the Home Credit Default Risk Kaggle competition\footnote{https://www.kaggle.com/c/home-credit-default-risk}, one of the popular scripts that analyzes a dataset of 150 MB, generates up to 10 GB of artifacts.
\todo[inline]{10 GB is a rough estimate, I need to finish the code to accurately calculate the number}
Therefore, materializing every artifact is not feasible.
In this section, we discuss our algorithm for materializing a subset of the artifacts under limited storage, a modified version of the algorithm presented by bhattacherjee et al. \cite{bhattacherjee2015principles}.
The goal of the algorithm is to materialize the artifacts that result in the lowest weighted recreation cost while ensuring the total size of the materialized artifacts does not exceed the storage capacity.
We define the weighted recreation cost of the graph $\mathcal{G}$ ($WC$), given the set of materialized vertices $\mathcal{MV}$ as 
\[
WC(\mathcal{G}, \mathcal{MV}) =  \sum\limits_{e \in \{e' \in \mathcal{E}  \lvert dest(e') \notin \mathcal{MV}\}}  e.f \times e.t
\]
where $dest(e)$ represent the destination vertex of the edge $e$.
The weighted recreation cost indicates how much time do we need to spend to execute all the operations in the graph, since the beginning of time.
For un-materialized artifacts, we must consider the frequency of the operations that produce the artifact.
For example, in Figure \ref{fig-experiment-graph}, if we do not materialize $v_4$ and the operation \textit{vectorizer.f\_t} has a frequency of 2, we must consider both executions of the operation when computing the weighted cost.
Whereas, if $v_4$ is materialized, the \textit{vectorizer.f\_t} operation has no impact on the weighted recreation cost.
%\todo[inline]{show that it is NP hard and cite \cite{bhattacherjee2015principles} and state the differences in their work and solution and ours}
%Bhattacherjee et al. \cite{bhattacherjee2015principles} tackle a similar problem and prove that the problem is NP-hard.
%They provide a greedy solution for the problem as well.
%However, there differences in both the use cases and problem formulation that make their solution not feasible in our case.

\begin{algorithm}[h]
\caption{Materialization of Artifacts}\label{algorithm-materialization}
\begin{algorithmic}[1]
\Require  $\mathcal{G(V,E)}=$ experiment graph, $\mathcal{B}=$ storage limit
\Ensure $\mathcal{MV}=$ set of materialized artifacts
\State $T=v_0.size$, $\mathcal{MV} =\{v_0\}$
\Do 
	\State $\mathcal{CV} = \{v \in \mathcal{V} \lvert v \notin \mathcal{MV}, T + v.s \leq \mathcal{B}\}$
	\State $v^* = \argmax\limits_{v \in \mathcal{CV}} \tfrac{\rho(\mathcal{G}, v)}{v.s}$
	\State $\mathcal{MV} = \mathcal{MV} \cup \{v^*\}$
	\State $T = T + v^*.size$
\DoWhile{$\mathcal{CV} \neq \emptyset$}
\end{algorithmic}
\end{algorithm}
Algorithm \ref{algorithm-materialization} shows the details of our method for selecting the vertices to materialize.
First, we start by initializing the materialized vertices set ($\mathcal{MV}$) to contain the root artifact ($v_0$) which represents the raw dataset.
This is essential as many of the feature engineering and model building operations are not invertible.
As a result, we cannot reconstruct the raw dataset if it is not materialized.
Then, while the storage limit is not reached, we materialize vertices with the maximum value of weighted recreation cost over size.
We compute the weighted recreation cost ($\rho$) of the vertex $v$ as, 
\[
\rho(\mathcal{G}, v) = \alpha(\mathcal{G}, v) \times \sum\limits_{e \in path(\mathcal{G}, v_0, v)}  e.t
\]
where $\alpha(\mathcal{G}, v)$ represents the access frequency of the vertex $v$ which is the same as frequency of the edge (or any of the edges in case of merge operation) connected to $v$.
The set $path(\mathcal{G}, v_0, v)$ represents the set of all edges from the root node to the vertex $v$. 
For example, in Figure \ref{fig-materialization-example}a, the recreation cost of $v_4$ is $3 \times (0 + 0 + 25 + 1 + 1) = 81$.
The ratio of the weighted recreation cost over size has the unit second per megabyte.
For example, the ratio 10 s/mb for an artifact, indicates that we need to spend 10 seconds to recreate 1 megabyte of the artifact.
Figure \ref{fig-materialization-example} shows an example of the materialization process when the storage capacity is 55.
For $v_0$ we do not compute $\rho$ as $v_0$ is always materialized.

\begin{figure}
\begin{subfigure}{0.5\linewidth}
\centering
\input{../images/tikz/graph-mat-step-1.tex}
\caption{Original Graph}
\end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
\input{../images/tikz/graph-mat-step-2.tex}
\caption{Materialized Graph}
\end{subfigure}
\begin{subfigure}{\linewidth}
\setlength\tabcolsep{3.5pt} % This is to ensure the table does not go out of bound
\begin{tabular}{l | | >{\bfseries}r | r  |>{\bfseries}r | r | r | >{\bfseries}r | >{\bfseries}r | >{\bfseries}r |>{\bfseries}r }
\hline
\textbf{vertex} & $\boldsymbol{v_0}$ & $v_1$ & $\boldsymbol{v_2}$ & $v_3$ & $v_4$ & $\boldsymbol{v_5}$ & $\boldsymbol{v_6}$ & $\boldsymbol{v_7}$ &$\boldsymbol{v_8}$ \\
\hline
\textbf{size (MB)}    & 10 & 8 & 2 & 40 & 42 & 1 & 30 & 2   & 3        \\
\textbf{$\boldsymbol{\rho}$ (s)} & ---   & 3 & 3 & 78 & 81 & 52 & 141 & 107 & 154	  \\
\textbf{ratio}& ---   & 0.37 & 1.5 & 1.95 & 1.93 & 52 & 4.7 & 53.5 & 51.3	\\
\hline
\end{tabular}
\caption{List of vertices, their sizes, recreation costs, and the cost over size ratio (Bold vertices are materialized).}
\end{subfigure}
\caption{Artifact materialization based on Algorithm \ref{algorithm-materialization} when storage capacity is 55 (MB)}
\label{fig-materialization-example}
\end{figure}

\subsubsection{The Effect of Model Quality on the Materialization Decision}
\todo[inline]{For this I need to design experiments to check if it has a positive impact. If we don't have space, then this will be moved to future work.}
Since the goal of all the workloads in the experiment graph is to solve the same task (as described in \ref{sec-introduction}), all the machine learning models will be evaluated using the same quality metric.
Therefore, we can utilize the quality of the model in the materialization algorithm.
We propose a simple method for utilizing the model quality in the materialization decision algorithm.
We start by adding a new attribute, $q$, to every edge.
The new attribute is computed as follows.
If the edge $e$ belongs to no path that leads to a predictive model, we assign $q$ to the average quality of all the predictive models in the experiment graph.
If $e$ belongs to only one path that leads to a predictive model, then we assign $q$ to the quality of the model.
If $e$ belongs to multiple paths that lead to different predictive models, we assign $q$ to the quality of the model with the maximum quality among all the models.
After computing $q$, we include it in the computation of $\rho$, by multiplying $e.q$ by $e.t$ in the summation. 

\subsection{Optimization Workflow}
Figure \ref{fig-system-workflow} shows the workflow of our system.
First, we transform the workload into its graph representation.
Then we utilize the experiment graph to optimize the workload using the techniques in Sections \ref{sec-reuse-and-warmstarting} and \ref{sec-hyperparam-optimization}.
This results in a new workload graph which depending on the types of optimizations may have a fewer number of operations, faster operations, or operation configurations that lead to higher quality machine learning models.
\begin{figure}
\centering
\input{../images/tikz/system-workflow.tex}
\caption{System workflow}
\label{fig-system-workflow}
\end{figure}
