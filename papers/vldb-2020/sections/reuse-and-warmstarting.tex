\section{Reuse and Warmstarting Optimizations}\label{sec-reuse-and-warmstarting}
With the experiment graph constructed and materialized, we can look for optimization opportunities for feature engineering and model training operations.
In this section, we propose two optimizations, namely, \textit{reuse}, \textit{warmstarting}.

\subsection{Reuse Optimization ML Operations}
We devise a strategy to detect which operations in the current workload exist in the experiment graph.
When an operation exists in the experiment graph, we directly access the resulting artifact instead of executing the operation.
Before executing a workload, we first transform it into its graph representation, which results in a directed graph, called $\mathcal{WG}$, that starts at $v_0$.
Then, we traverse the experiment graph starting from vertex $v_0$ using the edges of the workload graph $\mathcal{WG}$.
The result of the traversal is a subgraph of the experiment graph, which we refer to as $\mathcal{SG}$, which contains all the vertices and edges that exist in both the experiment graph and the workload graph.
For every path in $\mathcal{SG}$ which originates at $v_0$, we return the furthest materialized vertex from the root node as the result and skip all the intermediate operations.
\todo[inline]{Add a figure similar to the one from the offsite slides}

\subsection{Warmstarting Optimization For Model Training Operations}
Model training operations include extra hyperparameters that must be set before the training procedure begins.
Two training operations on the same data artifact using the same training algorithm could potentially have very different results based on the values of the hyperparameters.
Therefore, we cannot apply the reuse optimization in cases the hyperparameters are different.
Instead, we apply the \textit{warmstarting} optimization.
In our experiment graph, we refer to the set of every machine learning model that is trained on the same data artifact but only differs in hyperparameters, as a model group.
If a workload contains a model training operation, $e_{t}$, on a vertex, $v_{t}$, before executing the workload, we proceed as follows.
First, using the same traversal strategy as explained the reuse optimization, we find the vertex $v_{t}$ exist in the experiment graph.
Then, if $v_{t}$ exists in the experiment graph, we find the model group containing all the models are trained on $v_{t}$.
Finally, if the model group is not empty, we warmstart the operation $e_{t}$ with the best performing model from the model group.
\todo[inline]{Either add a figure are write an algorithm to explain the process}

Warmstarting can greatly reduce the total training time.
However, the type of the machine learning model and the termination criteria play important roles in determining the effect of the warmstarting optimization.
In the experiment section, we evaluate the effect of warmstarting on different types of models with different termination criteria.

\subsubsection{Augmenting the experiment graph}
When we utilize warmstarting, we extend the experiment graph with a merge operation which merges the dataset and the candidate model for warmstarting.
The actual training operation is then applied to the merged node.
As a result, we can keep track of the models that are utilized in warmstarting the training of other models which ensures reproducibility.

%For iterative training algorithms that are minimizing a loss function, there are two termination criteria, namely, the convergence tolerance and the number of iterations.
%
%\subsubsection{Convergence tolerance termination criteria}
%When the termination criteria of the model training operation in the workload is set to a specific convergence tolerance value, two scenarios may occur.
%In the first scenario, an existing trained model in the experiment graph has already reached the convergence tolerance value.
%In this scenario, we expect a large improvement in the training time as the training procedure in the workload will immediately converge.
%In the second scenario, no model in the experiment graph has reached the convergence tolerance value.
%In this case, we warmstart the model in the workload, to the model in the experiment graph with the highest attained quality.
%Therefore, we ensure the training procedure will converge faster.

%\subsubsection{Augmenting the experiment graph}
%Once the training procedure is finished, we augment the experiment graph with an edge and node representing the new model building operation and resulting model, respectively.
%\todo[inline]{We may need a special edge so that we know the training operation was not run from scratch and is the result of warmstarting.}

%\subsubsection{Partial Warmstarting Optimization For Model Training Operations}
%A common approach in machine learning workloads is to repeatedly select a different subset of features or create new features from the existing ones and train models on the new features.
%As a result, many model training operations operate on overlapping or different set of features.
%In the partial warmstarting optimization, we aim to improve the training time (and the quality) by warmstarting only the features that exist in the experiment database.

%\subsection{Reuse Optimization for Model Building Operations}
%Reuse for model building operations is more complicated.
%There are two types of reuse opportunities in the model building operations.
%
%\subsubsection{Exact Reuse}\label{sub-sub-exact-reuse}
%For non-user-defined aggregation operations, we follow the same procedure as the feature engineering processes.
%When the corresponding edge in the experiment graph has the same vertex and (aggregation) operation type, we reuse the result of the operation directly.
%We can also reuse the existing model training operation, if the input columns, algorithm, and all the hyper-parameters are the same.
%
%\subsubsection{Model parameter and hyper parameter warmstarting}\label{sub-sub-model-reuse}
%For the model training operations, 3 scenarios can occur.
%In the \textit{first scenario}, the training algorithm used for training the model has never been used before, therefore no meta-data about it exists in the experiment graph.
%In this scenario, no optimization is possible and the model training operation has to be executed completely.
%In the \textit{second scenario}, the training algorithm and the input columns to the model already exist in the experiment graph, but the specific hyperparameter setting does not.
%In this scenario, we can warmstart the model using the parameters from the corresponding node in the experiment graph.
%This reduces the training time as the model \hl{may} converge faster.
%\todo[inline]{This requires experiment and some math ?}
%In the \textit{third scenario}, the training algorithm and the hyperparameters are the same, but all the input columns do not exist in the corresponding node in the experiment graph.
%In this scenario, we provide partial warmstarting.
%In partial warmstarting, the model parameters corresponding to the columns of the input data that already exist in the experiment graph are warmstarted, and the rest of the parameters are randomly initialized.
%\todo[inline]{This requires experiment and some math ?}


%\subsection{Materialization of Grid Search}
%\todo[inline]{Incomplete}
%In order to analyze whether or not we should materialize parts of the grid search, we first have to unpack it, and compare it with other grid search.
%Then, similar to Section \ref{sub-sec-materialization-of-transformed-data}, we materialize the parts that are executed frequently.
%
%%\subsection{Guided Grid-Search}
%%\todo[inline]{just an idea}
%%By extracting correlation between different parameters and the model quality we can provided a guided grid search, where we can provide some estimate or show the effects of a hyperparameter range on the model quality