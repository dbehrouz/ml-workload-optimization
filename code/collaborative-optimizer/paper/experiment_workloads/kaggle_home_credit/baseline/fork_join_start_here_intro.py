#!/usr/bin/env python

"""Fork Baseline Workload 1 and Workload 2

This script combines 'start_here_a_gentle_introduction' and 'introduction_to_manual_feature_engineering'

Description:
    The workload joins the app_train_domain data set of 'start_here_a_gentle_introduction'
    with 'train_corrs_removed' dataset of introduction_to_manual_feature_engineering
    and trains a lgbm classifier.

Necessary modifications to make the script work with Experiment Graph:
    1. A test dataset is generated by splitting the original data
    2. Cross validation is replaced with full training
"""
import os
# File system management
# Suppress warnings
import warnings
# plotting libraries
from datetime import datetime

import matplotlib

from experiment_graph.executor import BaselineExecutor
from experiment_graph.workload import Workload

matplotlib.use('ps')
import matplotlib.pyplot as plt
# numpy and pandas for data manipulation
import numpy as np
import pandas as pd
import seaborn as sns
# sklearn preprocessing for dealing with categorical variables
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder

warnings.filterwarnings('ignore')


class fork_join_here_intro(Workload):

    def run(self, root_data):
        print(os.listdir(root_data))
        app_train = pd.read_csv(root_data + '/kaggle_home_credit/application_train.csv')
        print('Training data shape: ', app_train.shape)
        app_train.head()

        app_test = pd.read_csv(root_data + '/kaggle_home_credit/application_test.csv')
        print('Testing data shape: ', app_test.shape)
        app_test.head()

        app_train['TARGET'].value_counts()

        app_train['TARGET'].astype(int).plot.hist()

        test_labels = pd.read_csv(root_data + '/kaggle_home_credit/application_test_labels.csv')

        # Function to calculate missing values by column# Funct
        def missing_values_table(df):
            # Total missing values
            mis_val = df.isnull().sum()

            # Percentage of missing values
            mis_val_percent = 100 * df.isnull().sum() / len(df)

            # Make a table with the results
            mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

            # Rename the columns
            mis_val_table_ren_columns = mis_val_table.rename(columns={
                0: 'Missing Values',
                1: '% of Total Values'
            })

            # Sort the table by percentage of missing descending
            mis_val_table_ren_columns = mis_val_table_ren_columns[
                mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(
                '% of Total Values', ascending=False).round(1)

            # Print some summary information
            print("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
                                                                      "There are " + str(
                mis_val_table_ren_columns.shape[0]) +
                  " columns that have missing values.")

            # Return the dataframe with missing information
            return mis_val_table_ren_columns

        missing_values = missing_values_table(app_train)
        missing_values.head(20)

        app_train.dtypes.value_counts()

        app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)

        # Create a label encoder object
        le = LabelEncoder()
        le_count = 0

        # Iterate through the columns
        for col in app_train:
            if app_train[col].dtype == 'object':
                # If 2 or fewer unique categories
                if len(list(app_train[col].unique())) <= 2:
                    # Train on the training data
                    le.fit(app_train[col])
                    # Transform both training and testing data
                    app_train[col] = le.transform(app_train[col])
                    app_test[col] = le.transform(app_test[col])

                    # Keep track of how many columns were label encoded
                    le_count += 1

        print('%d columns were label encoded.' % le_count)

        # one-hot encoding of categorical variables
        app_train = pd.get_dummies(app_train)
        app_test = pd.get_dummies(app_test)

        print('Training Features shape: ', app_train.shape)
        print('Testing Features shape: ', app_test.shape)

        train_labels = app_train['TARGET']

        # Align the training and testing data, keep only columns present in both dataframes
        app_train, app_test = app_train.align(app_test, join='inner', axis=1)

        # Add the target back in
        app_train['TARGET'] = train_labels

        print('Training Features shape: ', app_train.shape)
        print('Testing Features shape: ', app_test.shape)

        (app_train['DAYS_BIRTH'] / -365).describe()

        app_train['DAYS_EMPLOYED'].describe()

        app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')
        plt.xlabel('Days Employment')

        anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]
        non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]
        print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))
        print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))
        print('There are %d anomalous days of employment' % len(anom))

        # Create an anomalous flag column
        app_train['DAYS_EMPLOYED_ANOM'] = app_train["DAYS_EMPLOYED"] == 365243

        # Replace the anomalous values with nan
        app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)

        app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')
        plt.xlabel('Days Employment')

        app_test['DAYS_EMPLOYED_ANOM'] = app_test["DAYS_EMPLOYED"] == 365243
        app_test["DAYS_EMPLOYED"].replace({365243: np.nan}, inplace=True)
        print('There are %d anomalies in the test data out of %d entries' % (
            app_test["DAYS_EMPLOYED_ANOM"].sum(), len(app_test)))

        # Find correlations with the target and sort
        correlations = app_train.corr()['TARGET'].sort_values()

        # Display correlations
        print('Most Positive Correlations:\n', correlations.tail(15))
        print('\nMost Negative Correlations:\n', correlations.head(15))

        # Find the correlation of the positive days since birth and target
        app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])
        app_train['DAYS_BIRTH'].corr(app_train['TARGET'])

        # Set the style of plots
        plt.style.use('fivethirtyeight')

        # Plot the distribution of ages in years
        plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor='k', bins=25)
        plt.title('Age of Client')
        plt.xlabel('Age (years)')
        plt.ylabel('Count')

        plt.figure(figsize=(10, 8))

        # KDE plot of loans that were repaid on time
        sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label='target == 0')

        # KDE plot of loans which were not repaid on time
        sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label='target == 1')

        # Labeling of plot
        plt.xlabel('Age (years)')
        plt.ylabel('Density')
        plt.title('Distribution of Ages')

        # Age information into a separate dataframe
        age_data = app_train[['TARGET', 'DAYS_BIRTH']]
        age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365

        # Bin the age data
        age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=np.linspace(20, 70, num=11))
        age_data.head(10)

        # Group by the bin and calculate averages
        age_groups = age_data.groupby('YEARS_BINNED').mean()
        age_groups

        plt.figure(figsize=(8, 8))

        # Graph the age bins and the average of the target as a bar plot
        plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])

        # Plot labeling
        plt.xticks(rotation=75)
        plt.xlabel('Age Group (years)')
        plt.ylabel('Failure to Repay (%)')
        plt.title('Failure to Repay by Age Group')

        # Extract the EXT_SOURCE variables and show correlations
        ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]
        ext_data_corrs = ext_data.corr()
        ext_data_corrs

        plt.figure(figsize=(8, 6))

        # Heatmap of correlations
        sns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)
        plt.title('Correlation Heatmap')

        plt.figure(figsize=(10, 12))

        # iterate through the sources
        for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):
            # create a new subplot for each source
            plt.subplot(3, 1, i + 1)
            # plot repaid loans
            sns.kdeplot(app_train[source][(app_train[source].notna()) & (app_train['TARGET'] == 0)],
                        label='target == 0')
            # plot loans that were not repaid
            sns.kdeplot(app_train[source][(app_train[source].notna()) & (app_train['TARGET'] == 1)],
                        label='target == 1')

            # Label the plots
            plt.title('Distribution of %s by Target Value' % source)
            plt.xlabel('%s' % source);
            plt.ylabel('Density');

        plt.tight_layout(h_pad=2.5)

        # Copy the data for plotting
        plot_data = ext_data.drop(columns=['DAYS_BIRTH']).copy()

        # Add in the age of the client in years
        plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']

        # Drop na values and limit to first 100000 rows
        plot_data = plot_data.dropna().loc[:100000, :]

        # Create the pairgrid object
        grid = sns.PairGrid(data=plot_data, size=3, diag_sharey=False,
                            hue='TARGET',
                            vars=[x for x in list(plot_data.columns) if x != 'TARGET'])

        # Upper is a scatter plot
        grid.map_upper(plt.scatter, alpha=0.2)

        # Diagonal is a histogram
        grid.map_diag(sns.kdeplot)

        # Bottom is density plot
        grid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)

        plt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05)

        # Make a new dataframe for polynomial features

        app_train_domain = app_train.copy()
        app_test_domain = app_test.copy()

        app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain[
            'AMT_INCOME_TOTAL']
        app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain[
            'AMT_INCOME_TOTAL']
        app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']
        app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']

        app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']
        app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']
        app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']
        app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']

        app_train_domain = app_train_domain.drop(columns='TARGET')

        import lightgbm as lgb

        def model(features, test_features, encoding='ohe', n_folds=5):

            """Train and test a light gradient boosting model using
            cross validation.

            Parameters
            --------
                features (pd.DataFrame):
                    dataframe of training features to use
                    for training a model. Must include the TARGET column.
                test_features (pd.DataFrame):
                    dataframe of testing features to use
                    for making predictions with the model.
                encoding (str, default = 'ohe'):
                    method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding
                    n_folds (int, default = 5): number of folds to use for cross validation

            Return
            --------
                submission (pd.DataFrame):
                    dataframe with `SK_ID_CURR` and `TARGET` probabilities
                    predicted by the model.
                feature_importances (pd.DataFrame):
                    dataframe with the feature importances from the model.
                valid_metrics (pd.DataFrame):
                    dataframe with training and validation metrics (ROC AUC) for each fold and overall.

            """

            # Extract the labels for training
            labels = features['TARGET']

            # Remove the ids and target
            features = features.drop(columns=['SK_ID_CURR', 'TARGET'])
            test_features = test_features.drop(columns=['SK_ID_CURR'])

            # One Hot Encoding
            if encoding == 'ohe':
                features = pd.get_dummies(features)
                test_features = pd.get_dummies(test_features)

                # Align the dataframes by the columns
                features, test_features = features.align(test_features, join='inner', axis=1)

                # No categorical indices to record
                cat_indices = 'auto'

            # Integer label encoding
            elif encoding == 'le':

                # Create a label encoder
                label_encoder = LabelEncoder()

                # List for storing categorical indices
                cat_indices = []

                # Iterate through each column
                for i, col in enumerate(features):
                    if features[col].dtype == 'object':
                        # Map the categorical features to integers
                        features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))
                        test_features[col] = label_encoder.transform(
                            np.array(test_features[col].astype(str)).reshape((-1,)))

                        # Record the categorical indices
                        cat_indices.append(i)

            # Catch error if label encoding scheme is not valid
            else:
                raise ValueError("Encoding must be either 'ohe' or 'le'")

            print('Training Data Shape: ', features.shape)
            print('Testing Data Shape: ', test_features.shape)

            # Create the model
            model = lgb.LGBMClassifier(objective='binary',
                                       class_weight='balanced', learning_rate=0.05,
                                       reg_alpha=0.1, reg_lambda=0.1,
                                       subsample=0.8, n_jobs=-1, random_state=50)

            # Train the model
            model.fit(features, labels, eval_metric='auc',
                      categorical_feature=cat_indices,
                      verbose=200)

            # Record the best iteration
            best_iteration = model.best_iteration_
            predictions = model.predict_proba(test_features, num_iteration=best_iteration)[:, 1]
            score = roc_auc_score(test_labels['TARGET'], predictions)
            print 'LGBMClassifier with AUC score: {}'.format(score)
            # Record the feature importances

        def agg_numeric(df, group_var, df_name):
            """Aggregates the numeric values in a dataframe. This can
            be used to create features for each instance of the grouping variable.

            Parameters
            --------
                df (dataframe):
                    the dataframe to calculate the statistics on
                group_var (string):
                    the variable by which to group df
                df_name (string):
                    the variable used to rename the columns

            Return
            --------
                agg (dataframe):
                    a dataframe with the statistics aggregated for
                    all numeric columns. Each instance of the grouping variable will have
                    the statistics (mean, min, max, sum; currently supported) calculated.
                    The columns are also renamed to keep track of features created.

            """
            # Remove id variables other than grouping variable
            for col in df:
                if col != group_var and 'SK_ID' in col:
                    df = df.drop(columns=col)

            group_ids = df[group_var]
            numeric_df = df.select_dtypes('number')
            numeric_df[group_var] = group_ids

            # Group by the specified variable and calculate the statistics
            agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()

            # Need to create new column names
            columns = [group_var]

            # Iterate through the variables names
            for var in agg.columns.levels[0]:
                # Skip the grouping variable
                if var != group_var:
                    # Iterate through the stat names
                    for stat in agg.columns.levels[1][:-1]:
                        # Make a new column name for the variable and stat
                        columns.append('%s_%s_%s' % (df_name, var, stat))

            agg.columns = columns
            return agg

        def count_categorical(df, group_var, df_name):
            """Computes counts and normalized counts for each observation
            of `group_var` of each unique category in every categorical variable

            Parameters
            --------
            df : dataframe
                The dataframe to calculate the value counts for.

            group_var : string
                The variable by which to group the dataframe. For each unique
                value of this variable, the final dataframe will have one row

            df_name : string
                Variable added to the front of column names to keep track of columns


            Return
            --------
            categorical : dataframe
                A dataframe with counts and normalized counts of each unique category in every categorical variable
                with one row for every unique value of the `group_var`.

            """

            # Select the categorical columns
            categorical = pd.get_dummies(df.select_dtypes('object'))

            # Make sure to put the identifying id on the column
            categorical[group_var] = df[group_var]

            # Groupby the group var and calculate the sum and mean
            categorical = categorical.groupby(group_var).agg(['sum', 'mean'])

            column_names = []

            # Iterate through the columns in level 0
            for var in categorical.columns.levels[0]:
                # Iterate through the stats in level 1
                for stat in ['count', 'count_norm']:
                    # Make a new column name
                    column_names.append('%s_%s_%s' % (df_name, var, stat))

            categorical.columns = column_names

            return categorical

        # Read in new copies of all the dataframes
        train = pd.read_csv(root_data + '/kaggle_home_credit/application_train.csv')
        bureau = pd.read_csv(root_data + '/kaggle_home_credit/bureau.csv')
        bureau_balance = pd.read_csv(root_data + '/kaggle_home_credit/bureau_balance.csv')

        bureau_counts = count_categorical(bureau, group_var='SK_ID_CURR', df_name='bureau')
        bureau_counts.head()

        bureau_agg = agg_numeric(bureau.drop(columns=['SK_ID_BUREAU']), group_var='SK_ID_CURR', df_name='bureau')
        bureau_agg.head()

        bureau_balance_counts = count_categorical(bureau_balance, group_var='SK_ID_BUREAU',
                                                  df_name='bureau_balance')
        bureau_balance_counts.head()

        bureau_balance_agg = agg_numeric(bureau_balance, group_var='SK_ID_BUREAU', df_name='bureau_balance')
        bureau_balance_agg.head()

        # Dataframe grouped by the loan
        bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index=True, left_on='SK_ID_BUREAU',
                                                  how='outer')

        # Merge to include the SK_ID_CURR
        bureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on='SK_ID_BUREAU', how='left')

        # Aggregate the stats for each client
        bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns=['SK_ID_BUREAU']),
                                               group_var='SK_ID_CURR',
                                               df_name='client')

        original_features = list(train.columns)
        print('Original Number of Features: ', len(original_features))

        # Merge with the value counts of bureau
        train = train.merge(bureau_counts, on='SK_ID_CURR', how='left')

        # Merge with the stats of bureau
        train = train.merge(bureau_agg, on='SK_ID_CURR', how='left')

        # Merge with the monthly information grouped by client
        train = train.merge(bureau_balance_by_client, on='SK_ID_CURR', how='left')

        new_features = list(train.columns)
        print('Number of features using previous loans from other institutions data: ', len(new_features))

        # Function to calculate missing values by column# Funct
        def missing_values_table(df):
            # Total missing values
            mis_val = df.isnull().sum()

            # Percentage of missing values
            mis_val_percent = 100 * df.isnull().sum() / len(df)

            # Make a table with the results
            mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

            # Rename the columns
            mis_val_table_ren_columns = mis_val_table.rename(
                columns={0: 'Missing Values', 1: '% of Total Values'})

            # Sort the table by percentage of missing descending
            mis_val_table_ren_columns = mis_val_table_ren_columns[
                mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(
                '% of Total Values', ascending=False).round(1)

            # Print some summary information
            print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
                                                                       "There are " + str(
                mis_val_table_ren_columns.shape[0]) +
                   " columns that have missing values.")

            # Return the dataframe with missing information
            return mis_val_table_ren_columns

        missing_train = missing_values_table(train)
        missing_train.head(10)

        missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])
        len(missing_train_vars)

        # Read in the test dataframe
        test = pd.read_csv(root_data + '/kaggle_home_credit/application_test.csv')

        test_labels = pd.read_csv(root_data + '/kaggle_home_credit/application_test_labels.csv')

        # Merge with the value counts of bureau
        test = test.merge(bureau_counts, on='SK_ID_CURR', how='left')

        # Merge with the stats of bureau
        test = test.merge(bureau_agg, on='SK_ID_CURR', how='left')

        # Merge with the value counts of bureau balance
        test = test.merge(bureau_balance_by_client, on='SK_ID_CURR', how='left')

        print('Shape of Testing Data: ', test.shape)

        train_labels = train['TARGET']

        # Align the dataframes, this will remove the 'TARGET' column
        train, test = train.align(test, join='inner', axis=1)

        train['TARGET'] = train_labels

        print('Training Data Shape: ', train.shape)
        print('Testing Data Shape: ', test.shape)

        missing_test = missing_values_table(test)
        missing_test.head(10)

        missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])
        len(missing_test_vars)

        missing_columns = list(set(missing_test_vars + missing_train_vars))
        print('There are %d columns with more than 90%% missing in either the training or testing data.' % len(
            missing_columns))

        # Drop the missing columns
        train = train.drop(columns=missing_columns)
        test = test.drop(columns=missing_columns)

        # train.to_csv('train_bureau_raw.csv', index=False)
        # test.to_csv('test_bureau_raw.csv', index=False)

        # Calculate all correlations in dataframe
        corrs = train.corr()

        corrs = corrs.sort_values('TARGET', ascending=False)

        # Ten most positive correlations
        print(pd.DataFrame(corrs['TARGET'].head(10)))

        # Ten most negative correlations
        print(pd.DataFrame(corrs['TARGET'].dropna().tail(10)))

        # TODO: even on the kaggle kernel the two lines have errors
        # kde_target(var_name='client_bureau_balance_counts_mean', df=train)
        #
        # kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df=train)

        # Set the threshold
        threshold = 0.8

        # Empty dictionary to hold correlated variables
        above_threshold_vars = {}

        # For each column, record the variables that are above the threshold
        for col in corrs:
            above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])

        # Track columns to remove and columns already examined
        cols_to_remove = []
        cols_seen = []
        cols_to_remove_pair = []

        # Iterate through columns and correlated columns
        for key, value in above_threshold_vars.items():
            # Keep track of columns already examined
            cols_seen.append(key)
            for x in value:
                if x == key:
                    next
                else:
                    # Only want to remove one in a pair
                    if x not in cols_seen:
                        cols_to_remove.append(x)
                        cols_to_remove_pair.append(key)

        cols_to_remove = list(set(cols_to_remove))
        if 'SK_ID_CURR' in cols_to_remove:
            cols_to_remove.remove('SK_ID_CURR')
        print('Number of columns to remove: ', len(cols_to_remove))

        train_corrs_removed = train.drop(columns=cols_to_remove)
        test_corrs_removed = test.drop(columns=cols_to_remove)

        print('Training Corrs Removed Shape: ', train_corrs_removed.shape)
        print('Testing Corrs Removed Shape: ', test_corrs_removed.shape)

        # Merge with the value counts of bureau
        final_train = train_corrs_removed.merge(app_train_domain, on='SK_ID_CURR', how='left')
        final_test = test_corrs_removed.merge(app_test_domain, on='SK_ID_CURR', how='left')

        # Test the domain knowledge features
        model(final_train, final_test)
        # LGBMClassifier with AUC score: 0.767512274394

        return True


if __name__ == "__main__":
    ROOT = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization'
    ROOT_PACKAGE = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer'
    root_data = ROOT + '/data'
    import sys

    sys.path.append(ROOT_PACKAGE)

    executor = BaselineExecutor()
    execution_start = datetime.now()
    workload = fork_join_here_intro()
    executor.end_to_end_run(workload=workload, root_data=root_data)

    execution_end = datetime.now()
    elapsed = (execution_end - execution_start).total_seconds()

    print('finished execution in {} seconds'.format(elapsed))
