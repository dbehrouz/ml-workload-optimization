{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import networkx as nx\n",
    "import os\n",
    "import warnings\n",
    "# matplotlib and seaborn for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "ROOT_PACKAGE_DIRECTORY = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter'\n",
    "ROOT_DATA_DIRECTORY = ROOT_PACKAGE_DIRECTORY + '/data'\n",
    "GRAPH_DATABASE_PATH = ROOT_PACKAGE_DIRECTORY + '/data/graph/exp-reuse-same-workload.graph'\n",
    "sys.path.append(ROOT_PACKAGE_DIRECTORY)\n",
    "\n",
    "# Experiment Graph\n",
    "from experiment_graph.execution_environment import ExecutionEnvironment\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "try:\n",
    "    import pygraphviz\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "except ImportError:\n",
    "    try:\n",
    "        import pydot\n",
    "        from networkx.drawing.nx_pydot import graphviz_layout\n",
    "    except ImportError:\n",
    "        raise ImportError(\"This example needs Graphviz and either \"\n",
    "                          \"PyGraphviz or pydot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import hashlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Experiment Graph\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from importlib import import_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = ROOT_PACKAGE_DIRECTORY + '/data'\n",
    "DATABASE_PATH = ROOT_PACKAGE_DIRECTORY + '/data/environment_same_workload'\n",
    "execution_environment = ExecutionEnvironment('naive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bureau = execution_environment.load(root_data + '/home-credit-default-risk/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: \\n', array([ 938.23786125]))\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients: \\n', regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5715448</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_BUREAU  MONTHS_BALANCE STATUS\n",
       "0       5715448               0      C\n",
       "1       5715448              -1      C\n",
       "2       5715448              -2      C\n",
       "3       5715448              -3      C\n",
       "4       5715448              -4      C"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in bureau balance\n",
    "bureau_balance = execution_environment.load(root_data + '/home-credit-default-risk/bureau_balance.csv')\n",
    "bureau_balance.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'e7c6b65f56f09d375806de4f5f6d4aea'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a8ccfc2b15c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbureau_balance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_result\u001b[0;34m(self, v_id, verbose)\u001b[0m\n\u001b[1;32m    160\u001b[0m                         \u001b[0;31m# TODO: check if a shallow copy is enough\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         cur_node['data'].c_name, cur_node['data'].c_hash = copy.deepcopy(\n\u001b[0;32m--> 162\u001b[0;31m                             self.compute_next(self.graph.nodes[pair[0]], edge))\n\u001b[0m\u001b[1;32m    163\u001b[0m                         \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# all the other node types they contain the data themselves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_next\u001b[0;34m(node, edge)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oper'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mp_head\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mp_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_and_store_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/data_storage.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, names, hashes)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mcombined_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'e7c6b65f56f09d375806de4f5f6d4aea'"
     ]
    }
   ],
   "source": [
    "bureau_balance.select_dtypes('object').head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 'e7c6b65f56f09d375806de4f5f6d4aea' already exist\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'b8d67a3152485c9805340c4a79616203_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d0ee739a7653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbureau_balance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.pyc\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_result\u001b[0;34m(self, v_id, verbose)\u001b[0m\n\u001b[1;32m    161\u001b[0m                         cur_node['data'].c_name, cur_node['data'].c_hash = copy.deepcopy(\n\u001b[1;32m    162\u001b[0m                             self.compute_next(self.graph.nodes[pair[0]], edge))\n\u001b[0;32m--> 163\u001b[0;31m                         \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# all the other node types they contain the data themselves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.pyc\u001b[0m in \u001b[0;36mcompute_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/data_storage.pyc\u001b[0m in \u001b[0;36mget_size\u001b[0;34m(self, column_hashes)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mcombined_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_hashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_hash\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b8d67a3152485c9805340c4a79616203_size'"
     ]
    }
   ],
   "source": [
    "bureau_balance.select_dtypes('object').data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'b8d67a3152485c9805340c4a79616203_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-643e93f2ccb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Counts of each type of status for each previous loan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbureau_balance_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbureau_balance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SK_ID_BUREAU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bureau_balance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbureau_balance_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-48b80d707b2b>\u001b[0m in \u001b[0;36mcount_categorical\u001b[0;34m(df, group_var, df_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroup_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Need to create new column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgroup_var\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_result\u001b[0;34m(self, v_id, verbose)\u001b[0m\n\u001b[1;32m    161\u001b[0m                         cur_node['data'].c_name, cur_node['data'].c_hash = copy.deepcopy(\n\u001b[1;32m    162\u001b[0m                             self.compute_next(self.graph.nodes[pair[0]], edge))\n\u001b[0;32m--> 163\u001b[0;31m                         \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# all the other node types they contain the data themselves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mcompute_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/data_storage.py\u001b[0m in \u001b[0;36mget_size\u001b[0;34m(self, column_hashes)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mcombined_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_hashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_hash\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b8d67a3152485c9805340c4a79616203_size'"
     ]
    }
   ],
   "source": [
    "# Counts of each type of status for each previous loan\n",
    "bureau_balance_counts = count_categorical(bureau_balance, group_var='SK_ID_BUREAU', df_name='bureau_balance')\n",
    "bureau_balance_counts.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SK_ID_CURR'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau['SK_ID_CURR'].c_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'640e1ddef2c2af1a9a0ff5b77797795b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-54266ef74fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecution_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'640e1ddef2c2af1a9a0ff5b77797795b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '640e1ddef2c2af1a9a0ff5b77797795b'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.c_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encode.add_columns('SK_ID_CURR', bureau['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.c_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ead040746fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'previous_loan_counts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'previous_loan_counts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train = train.replace_columns('previous_loan_counts', train['previous_loan_counts'].fillna(0))\n",
    "train.head().data()\n",
    "\n",
    "# Plots the distribution of a variable colored by value of the target\n",
    "def kde_target(var_name, df):\n",
    "    # Calculate the correlation coefficient between the new variable and the target\n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "\n",
    "    # Calculate medians for repaid vs not repaid\n",
    "    avg_repaid = df[df['TARGET'] == 0][var_name].median()\n",
    "    avg_not_repaid = df[df['TARGET'] == 1][var_name].median()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the distribution for target == 0 and target == 1\n",
    "    sns.kdeplot(df[df['TARGET'] == 0][var_name].dropna().data(), label='TARGET == 0')\n",
    "    sns.kdeplot(df[df['TARGET'] == 1][var_name].dropna().data(), label='TARGET == 1')\n",
    "\n",
    "    # label the plot\n",
    "    plt.xlabel(var_name)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('%s Distribution' % var_name)\n",
    "    plt.legend()\n",
    "\n",
    "    # print out the correlation\n",
    "    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr.data()))\n",
    "    # Print out average values\n",
    "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid.data())\n",
    "    print('Median value for loan that was repaid =     %0.4f' % avg_repaid.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_target('EXT_SOURCE_3', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['previous_loan_counts'].data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace_columns('previous_loan_counts', train['previous_loan_counts'].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(dataset):\n",
    "    # Total missing values\n",
    "    mis_val = dataset.isnull().sum().data()\n",
    "\n",
    "    mis_val_percent = 100 * mis_val / len(dataset.data())\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns={\n",
    "        0: 'Missing Values',\n",
    "        1: '% of Total Values'\n",
    "    })\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(dataset.shape().data()[1]) + \" columns.\\n\"\n",
    "                                                                          \"There are \" + str(\n",
    "        mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.dtypes().data().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.select_dtypes('object').nunique().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "columns = app_train.select_dtypes('object').data().columns\n",
    "for col in columns:\n",
    "    # we are not using nunique because it discard nan\n",
    "    if app_train[col].nunique(dropna=False).data() <= 2:\n",
    "        model = app_train[col].fit_sk_model(le)\n",
    "\n",
    "        transformed_train = model.transform_col(app_train[col], col)\n",
    "        app_train = app_train.drop(col)\n",
    "        app_train = app_train.add_columns(col, transformed_train)\n",
    "\n",
    "        transformed_test = model.transform_col(app_test[col], col)\n",
    "        app_test = app_test.drop(col)\n",
    "        app_test = app_test.add_columns(col, transformed_test)\n",
    "\n",
    "        # Keep track of how many columns were label encoded\n",
    "        le_count += 1\n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train = app_train.onehot_encode()\n",
    "app_test = app_test.onehot_encode()\n",
    "\n",
    "print('Training Features shape: ', app_train.shape().data())\n",
    "print('Testing Features shape: ', app_test.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "train_columns = app_train.data().columns\n",
    "test_columns = app_test.data().columns\n",
    "for c in train_columns:\n",
    "    if c not in test_columns:\n",
    "        app_train = app_train.drop(c)\n",
    "\n",
    "app_train = app_train.add_columns('TARGET', train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features shape: ', app_train.shape().data())\n",
    "print('Testing Features shape: ', app_test.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(app_train['DAYS_BIRTH'] / 365).describe().data()\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].describe().data()\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].data().plot.hist(title='Days Employment Histogram')\n",
    "plt.xlabel('Days Employment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
    "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean().data()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean().data()))\n",
    "print('There are %d anomalous days of employment' % anom.shape().data()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_employed_anom = app_train['DAYS_EMPLOYED'] == 365243\n",
    "app_train = app_train.add_columns('DAYS_EMPLOYED_ANOM', days_employed_anom)\n",
    "temp = app_train['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "app_train = app_train.drop('DAYS_EMPLOYED')\n",
    "app_train = app_train.add_columns('DAYS_EMPLOYED', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train[\"DAYS_EMPLOYED\"].data().plot.hist(title='Days Employment Histogram');\n",
    "plt.xlabel('Days Employment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_employed_anom = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test = app_test.add_columns('DAYS_EMPLOYED_ANOM', days_employed_anom)\n",
    "temp = app_test['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "app_test = app_test.drop('DAYS_EMPLOYED')\n",
    "app_test = app_test.add_columns('DAYS_EMPLOYED', temp)\n",
    "print('There are %d anomalies in the test data out of %d entries'\n",
    "      % (app_test['DAYS_EMPLOYED_ANOM'].sum().data(),\n",
    "         app_test.shape().data()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations = app_train.corr().data()\n",
    "# top = correlations['TARGET'].sort_values()\n",
    "# # Display correlations\n",
    "# print('Most Positive Correlations:\\n', top.tail(15))\n",
    "# print('\\nMost Negative Correlations:\\n', top.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_age = app_train['DAYS_BIRTH'].abs()\n",
    "app_train = app_train.drop('DAYS_BIRTH')\n",
    "app_train = app_train.add_columns('DAYS_BIRTH', abs_age)\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET']).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist((app_train['DAYS_BIRTH'] / 365).data(), edgecolor='k', bins=25)\n",
    "plt.title('Age of Client')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# # KDE plot of loans that were repaid on time\n",
    "# sns.kdeplot((app_train[app_train['TARGET'] == 0]['DAYS_BIRTH'] / 365).data(), label='target == 0')\n",
    "# # KDE plot of loans which were not repaid on time\n",
    "# sns.kdeplot((app_train[app_train['TARGET'] == 1]['DAYS_BIRTH'] / 365).data(), label='target == 1')\n",
    "# # Labeling of plot\n",
    "# plt.xlabel('Age (years)')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Distribution of Ages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "years_birth = age_data['DAYS_BIRTH'] / 365\n",
    "age_data = age_data.add_columns('YEARS_BIRTH', years_birth)\n",
    "binned = age_data['YEARS_BIRTH'].binning(20, 70, 11)\n",
    "binned.setname('YEARS_BINNED')\n",
    "age_data = age_data.add_columns('YEARS_BINNED', binned)\n",
    "age_data.head(10).data()\n",
    "\n",
    "age_groups = age_data.groupby('YEARS_BINNED').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.data().index.astype(str), age_groups.data()['TARGET'] * 100)\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation=75)\n",
    "plt.xlabel('Age Group (years)')\n",
    "plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "# ext_data_corrs = ext_data.corr().data()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Heatmap of correlations\n",
    "# sns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)\n",
    "# plt.title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 12))\n",
    "\n",
    "# # iterate through the sources\n",
    "# for i, column in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "#     # create a new subplot for each source\n",
    "#     plt.subplot(3, 1, i + 1)\n",
    "#     # plot repaid loans\n",
    "#     source_data = app_train[[column, 'TARGET']][app_train['TARGET'] == 0]\n",
    "#     sns.kdeplot(source_data[app_train[column].notna()][column].data(), label='target == 0')\n",
    "#     # plot loans that were not repaid\n",
    "#     source_data = app_train[[column, 'TARGET']][app_train['TARGET'] == 1]\n",
    "#     sns.kdeplot(source_data[app_train[column].notna()][column].data(), label='target == 1')\n",
    "\n",
    "#     # Label the plots\n",
    "#     plt.title('Distribution of %s by Target Value' % column)\n",
    "#     plt.xlabel('%s' % column)\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "# plt.tight_layout(h_pad=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy the data for plotting\n",
    "# plot_data = ext_data.drop('DAYS_BIRTH')\n",
    "\n",
    "# # Add in the age of the client in years\n",
    "# plot_data = plot_data.add_columns('YEARS_BIRTH', age_data['YEARS_BIRTH'])\n",
    "# # Drop na values and limit to first 100000 rows\n",
    "# plot_data = plot_data.head(100000).dropna()\n",
    "\n",
    "# # Create the pair grid object\n",
    "# grid = sns.PairGrid(data=plot_data.data(), size=3, diag_sharey=False,\n",
    "#                     hue='TARGET',\n",
    "#                     vars=[x for x in list(plot_data.data().columns) if x != 'TARGET'])\n",
    "\n",
    "# # Upper is a scatter plot\n",
    "# grid.map_upper(plt.scatter, alpha=0.2)\n",
    "\n",
    "# # Diagonal is a histogram\n",
    "# grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# # Bottom is density plot\n",
    "# grid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)\n",
    "\n",
    "# plt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns=['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "imputer_model = poly_features.fit_sk_model(imputer)\n",
    "poly_features = imputer_model.transform(poly_features)\n",
    "poly_features_test = imputer_model.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer_model = poly_features.fit_sk_model(poly_transformer)\n",
    "\n",
    "poly_features = poly_transformer_model.transform(poly_features)\n",
    "poly_features_test = poly_transformer_model.transform(poly_features_test)\n",
    "\n",
    "new_names = poly_transformer_model.data().get_feature_names(input_features=[\n",
    "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'\n",
    "])\n",
    "\n",
    "# Call manually before meta data update\n",
    "poly_features.set_columns(new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the target\n",
    "poly_features = poly_features.add_columns('TARGET', poly_target)\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr().data()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "print(poly_corrs.head(10))\n",
    "print(poly_corrs.tail(5))\n",
    "poly_features_test.set_columns(new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge polynomial features into training dataframe\n",
    "poly_features = poly_features.add_columns('SK_ID_CURR', app_train['SK_ID_CURR'])\n",
    "app_train_poly = app_train.merge(poly_features, on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test = poly_features_test.add_columns('SK_ID_CURR', app_test['SK_ID_CURR'])\n",
    "app_test_poly = app_test.merge(poly_features_test, on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the dataframes\n",
    "train_columns = app_train_poly.data().columns\n",
    "test_columns = app_test_poly.data().columns\n",
    "for c in train_columns:\n",
    "    if c not in test_columns:\n",
    "        app_train_poly = app_train_poly.drop(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ',\n",
    "      app_train_poly.shape().data())\n",
    "print('Testing data with polynomial features shape:  ',\n",
    "      app_test_poly.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'CREDIT_INCOME_PERCENT',\n",
    "    app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL'])\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'ANNUITY_INCOME_PERCENT',\n",
    "    app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL'])\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'CREDIT_TERM',\n",
    "    app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT'])\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'DAYS_EMPLOYED_PERCENT',\n",
    "    app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH'])\n",
    "\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'CREDIT_INCOME_PERCENT',\n",
    "    app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL'])\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'ANNUITY_INCOME_PERCENT',\n",
    "    app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL'])\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'CREDIT_TERM',\n",
    "    app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT'])\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'DAYS_EMPLOYED_PERCENT',\n",
    "    app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 20))\n",
    "# # iterate through the new features\n",
    "# for i, column in enumerate([\n",
    "#     'CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM',\n",
    "#     'DAYS_EMPLOYED_PERCENT'\n",
    "# ]):\n",
    "#     # create a new subplot for each source\n",
    "#     plt.subplot(4, 1, i + 1)\n",
    "#     # plot repaid loans\n",
    "#     negative = app_train_domain[[column, 'TARGET']][app_train['TARGET'] == 0]\n",
    "#     sns.kdeplot(\n",
    "#         negative[app_train_domain[column].notna()][column].data(),\n",
    "#         label='target == 0')\n",
    "#     # plot loans that were not repaid\n",
    "#     positive = app_train_domain[[column, 'TARGET']][app_train['TARGET'] == 1]\n",
    "#     sns.kdeplot(\n",
    "#         positive[app_train_domain[column].notna()][column].data(),\n",
    "#         label='target == 1')\n",
    "\n",
    "#     # Label the plots\n",
    "#     plt.title('Distribution of %s by Target Value' % column)\n",
    "#     plt.xlabel('%s' % column)\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "# plt.tight_layout(h_pad=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "# Drop the target from the training data\n",
    "columns = app_train.data().columns\n",
    "if 'TARGET' in columns:\n",
    "    train = app_train.drop(columns=['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "\n",
    "\n",
    "# Feature names\n",
    "features = list(train.data().columns)\n",
    "\n",
    "# Copy of the testing data\n",
    "test = app_test.copy()\n",
    "\n",
    "# Median imputation of missing values\n",
    "sk_imputer = Imputer(strategy='median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "sk_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer = train.fit_sk_model(sk_imputer)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler = train.fit_sk_model(sk_scaler)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape().data())\n",
    "print('Testing data shape: ', test.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "sk_log_reg = LogisticRegression(C=0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg = train.fit_sk_model_with_labels(sk_log_reg, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "# Make sure to select the second column only\n",
    "log_reg_pred = log_reg.predict_proba(test)[1]\n",
    "\n",
    "# Submission data\n",
    "log_reg_pred.setname('TARGET')\n",
    "submit = app_test['SK_ID_CURR'].concat(log_reg_pred)\n",
    "submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make the random forest classifier\n",
    "sk_random_forest = RandomForestClassifier(n_estimators=5, random_state=50, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Train on the training data\n",
    "random_forest = train.fit_sk_model_with_labels(sk_random_forest, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = random_forest.feature_importances(features)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest.predict_proba(test)[1]\n",
    "\n",
    "# Score = 0.678\n",
    "# Submission dataframe\n",
    "predictions.setname('TARGET')\n",
    "submit = app_test['SK_ID_CURR'].concat(predictions)\n",
    "submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly_features_names = list(app_train_poly.data().columns)\n",
    "\n",
    "# # Impute the polynomial features\n",
    "# sk_imputer = Imputer(strategy='median')\n",
    "# imputer = app_train_poly.fit_sk_model(sk_imputer)\n",
    "\n",
    "# poly_features = imputer.transform(app_train_poly)\n",
    "# poly_features_test = imputer.transform(app_test_poly)\n",
    "\n",
    "# # Scale the polynomial features\n",
    "# sk_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = poly_features.fit_sk_model(sk_scaler)\n",
    "\n",
    "# poly_features = scaler.transform(poly_features)\n",
    "# poly_features_test = scaler.transform(poly_features_test)\n",
    "\n",
    "# sk_random_forest_poly = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "# random_forest_poly = poly_features.fit_sk_model_with_labels(sk_random_forest_poly, train_labels)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# predictions = random_forest_poly.predict_proba(poly_features_test)[1]\n",
    "\n",
    "# # Score = 0.678\n",
    "# # Submission dataframe\n",
    "# predictions.setname('TARGET')\n",
    "# submit = app_test['SK_ID_CURR'].concat(predictions)\n",
    "# submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_train_domain = app_train_domain.drop(columns='TARGET')\n",
    "\n",
    "# domain_features_names = list(app_train_domain.data().columns)\n",
    "\n",
    "# # Impute the domainnomial features\n",
    "# sk_imputer = Imputer(strategy='median')\n",
    "# imputer = app_train_domain.fit_sk_model(sk_imputer)\n",
    "\n",
    "# domain_features = imputer.transform(app_train_domain)\n",
    "# domain_features_test = imputer.transform(app_test_domain)\n",
    "\n",
    "# # Scale the domainnomial features\n",
    "# sk_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = domain_features.fit_sk_model(sk_scaler)\n",
    "\n",
    "# domain_features = scaler.transform(domain_features)\n",
    "# domain_features_test = scaler.transform(domain_features_test)\n",
    "\n",
    "# # Train on the training data\n",
    "# sk_random_forest_domain = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "# random_forest_domain = domain_features.fit_sk_model_with_labels(sk_random_forest_domain, train_labels)\n",
    "\n",
    "# # Extract feature importances\n",
    "# feature_importances_domain = random_forest_domain.feature_importances(domain_features_names)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# predictions = random_forest_domain.predict_proba(domain_features_test)[1]\n",
    "\n",
    "# # Score = 0.678\n",
    "# # Make a submission dataframe\n",
    "# predictions.setname('TARGET')\n",
    "# submit = app_test['SK_ID_CURR'].concat(predictions)\n",
    "# submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called `importance\n",
    "\n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "\n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest)\n",
    "        with a column for normalized importance\n",
    "        \"\"\"\n",
    "\n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending=False)\n",
    "\n",
    "    # Normalize the feature importances to add up to one\n",
    "    df = df.add_columns('importance_normalized', df['importance'] / df['importance'].sum().data())\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.data().index[:15]))),\n",
    "            df['importance_normalized'].data().head(15),\n",
    "            align='center', edgecolor='k')\n",
    "\n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.data().index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].data().head(15))\n",
    "\n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance');\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Show the feature importances for the default features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)\n",
    "\n",
    "feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
