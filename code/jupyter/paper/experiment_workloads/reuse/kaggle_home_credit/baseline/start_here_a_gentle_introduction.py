#!/usr/bin/env python

"""Baseline workload 1 for Home Credit Default Risk Competition
   The code here, is the original code posted as a notebook for the Kaggle competitiong.
   The notebook can be found here: https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction

   For now, I removed the Kfold and Gradient Boosted Tree models
   TODO: Add Kfold and Gradient Boosted Tree
"""
import os
# File system management
# Suppress warnings
import warnings

# plotting libraries
import matplotlib.pyplot as plt
# numpy and pandas for data manipulation
import numpy as np
import pandas as pd
import seaborn as sns
# sklearn preprocessing for dealing with categorical variables
from sklearn.preprocessing import LabelEncoder

warnings.filterwarnings('ignore')


def run(root_data):
    print(os.listdir(root_data))
    app_train = pd.read_csv(root_data + '/home-credit-default-risk/application_train.csv')
    print('Training data shape: ', app_train.shape)
    app_train.head()

    app_test = pd.read_csv(root_data + '/home-credit-default-risk/application_test.csv')
    print('Testing data shape: ', app_test.shape)
    app_test.head()

    app_train['TARGET'].value_counts()

    app_train['TARGET'].astype(int).plot.hist()

    # Function to calculate missing values by column# Funct
    def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()

        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)

        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(columns={
            0: 'Missing Values',
            1: '% of Total Values'
        })

        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(
            '% of Total Values', ascending=False).round(1)

        # Print some summary information
        print("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
                                                                  "There are " + str(
            mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")

        # Return the dataframe with missing information
        return mis_val_table_ren_columns

    missing_values = missing_values_table(app_train)
    missing_values.head(20)

    app_train.dtypes.value_counts()

    app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)

    # Create a label encoder object
    le = LabelEncoder()
    le_count = 0

    # Iterate through the columns
    for col in app_train:
        if app_train[col].dtype == 'object':
            # If 2 or fewer unique categories
            if len(list(app_train[col].unique())) <= 2:
                # Train on the training data
                le.fit(app_train[col])
                # Transform both training and testing data
                app_train[col] = le.transform(app_train[col])
                app_test[col] = le.transform(app_test[col])

                # Keep track of how many columns were label encoded
                le_count += 1

    print('%d columns were label encoded.' % le_count)

    # one-hot encoding of categorical variables
    app_train = pd.get_dummies(app_train)
    app_test = pd.get_dummies(app_test)

    print('Training Features shape: ', app_train.shape)
    print('Testing Features shape: ', app_test.shape)

    train_labels = app_train['TARGET']

    # Align the training and testing data, keep only columns present in both dataframes
    app_train, app_test = app_train.align(app_test, join='inner', axis=1)

    # Add the target back in
    app_train['TARGET'] = train_labels

    print('Training Features shape: ', app_train.shape)
    print('Testing Features shape: ', app_test.shape)

    (app_train['DAYS_BIRTH'] / -365).describe()

    app_train['DAYS_EMPLOYED'].describe()

    app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')
    plt.xlabel('Days Employment')

    anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]
    non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]
    print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))
    print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))
    print('There are %d anomalous days of employment' % len(anom))

    # Create an anomalous flag column
    app_train['DAYS_EMPLOYED_ANOM'] = app_train["DAYS_EMPLOYED"] == 365243

    # Replace the anomalous values with nan
    app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)

    app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')
    plt.xlabel('Days Employment')

    app_test['DAYS_EMPLOYED_ANOM'] = app_test["DAYS_EMPLOYED"] == 365243
    app_test["DAYS_EMPLOYED"].replace({365243: np.nan}, inplace=True)
    print('There are %d anomalies in the test data out of %d entries' % (
        app_test["DAYS_EMPLOYED_ANOM"].sum(), len(app_test)))

    # Find correlations with the target and sort
    correlations = app_train.corr()['TARGET'].sort_values()

    # Display correlations
    print('Most Positive Correlations:\n', correlations.tail(15))
    print('\nMost Negative Correlations:\n', correlations.head(15))

    # Find the correlation of the positive days since birth and target
    app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])
    app_train['DAYS_BIRTH'].corr(app_train['TARGET'])

    # Set the style of plots
    plt.style.use('fivethirtyeight')

    # Plot the distribution of ages in years
    plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor='k', bins=25)
    plt.title('Age of Client')
    plt.xlabel('Age (years)')
    plt.ylabel('Count')

    plt.figure(figsize=(10, 8))

    # KDE plot of loans that were repaid on time
    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label='target == 0')

    # KDE plot of loans which were not repaid on time
    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label='target == 1')

    # Labeling of plot
    plt.xlabel('Age (years)')
    plt.ylabel('Density')
    plt.title('Distribution of Ages')

    # Age information into a separate dataframe
    age_data = app_train[['TARGET', 'DAYS_BIRTH']]
    age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365

    # Bin the age data
    age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=np.linspace(20, 70, num=11))
    age_data.head(10)

    # Group by the bin and calculate averages
    age_groups = age_data.groupby('YEARS_BINNED').mean()
    age_groups

    plt.figure(figsize=(8, 8))

    # Graph the age bins and the average of the target as a bar plot
    plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])

    # Plot labeling
    plt.xticks(rotation=75)
    plt.xlabel('Age Group (years)')
    plt.ylabel('Failure to Repay (%)')
    plt.title('Failure to Repay by Age Group')

    # Extract the EXT_SOURCE variables and show correlations
    ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]
    ext_data_corrs = ext_data.corr()
    ext_data_corrs

    plt.figure(figsize=(8, 6))

    # Heatmap of correlations
    sns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)
    plt.title('Correlation Heatmap')

    plt.figure(figsize=(10, 12))

    # iterate through the sources
    for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):
        # create a new subplot for each source
        plt.subplot(3, 1, i + 1)
        # plot repaid loans
        sns.kdeplot(app_train[source][(app_train[source].notna()) & (app_train['TARGET'] == 0)], label='target == 0')
        # plot loans that were not repaid
        sns.kdeplot(app_train[source][(app_train[source].notna()) & (app_train['TARGET'] == 1)], label='target == 1')

        # Label the plots
        plt.title('Distribution of %s by Target Value' % source)
        plt.xlabel('%s' % source);
        plt.ylabel('Density');

    plt.tight_layout(h_pad=2.5)

    # Copy the data for plotting
    plot_data = ext_data.drop(columns=['DAYS_BIRTH']).copy()

    # Add in the age of the client in years
    plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']

    # Drop na values and limit to first 100000 rows
    plot_data = plot_data.dropna().loc[:100000, :]

    # Create the pairgrid object
    grid = sns.PairGrid(data=plot_data, size=3, diag_sharey=False,
                        hue='TARGET',
                        vars=[x for x in list(plot_data.columns) if x != 'TARGET'])

    # Upper is a scatter plot
    grid.map_upper(plt.scatter, alpha=0.2)

    # Diagonal is a histogram
    grid.map_diag(sns.kdeplot)

    # Bottom is density plot
    grid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)

    plt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05)

    # Make a new dataframe for polynomial features
    poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]
    poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]

    # imputer for handling missing values
    from sklearn.preprocessing import Imputer

    imputer = Imputer(strategy='median')

    poly_target = poly_features['TARGET']

    poly_features = poly_features.drop(columns=['TARGET'])

    # Need to impute missing values
    poly_features = imputer.fit_transform(poly_features)
    poly_features_test = imputer.transform(poly_features_test)

    from sklearn.preprocessing import PolynomialFeatures

    # Create the polynomial object with specified degree
    poly_transformer = PolynomialFeatures(degree=3)

    # Train the polynomial features
    poly_transformer.fit(poly_features)

    # Transform the features
    poly_features = poly_transformer.transform(poly_features)
    poly_features_test = poly_transformer.transform(poly_features_test)
    print('Polynomial Features shape: ', poly_features.shape)

    poly_transformer.get_feature_names(input_features=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])

    # Create a dataframe of the features
    poly_features = pd.DataFrame(poly_features,
                                 columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',
                                                                             'EXT_SOURCE_3', 'DAYS_BIRTH']))

    # Add in the target
    poly_features['TARGET'] = poly_target

    # Find the correlations with the target
    poly_corrs = poly_features.corr()['TARGET'].sort_values()

    # Display most negative and most positive
    print(poly_corrs.head(10))
    print(poly_corrs.tail(5))

    # Put test features into dataframe
    poly_features_test = pd.DataFrame(poly_features_test,
                                      columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',
                                                                                  'EXT_SOURCE_3', 'DAYS_BIRTH']))

    # Merge polynomial features into training dataframe
    poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']
    app_train_poly = app_train.merge(poly_features, on='SK_ID_CURR', how='left')

    # Merge polnomial features into testing dataframe
    poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']
    app_test_poly = app_test.merge(poly_features_test, on='SK_ID_CURR', how='left')

    # Align the dataframes
    app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join='inner', axis=1)

    # Print out the new shapes
    print('Training data with polynomial features shape: ', app_train_poly.shape)
    print('Testing data with polynomial features shape:  ', app_test_poly.shape)

    app_train_domain = app_train.copy()
    app_test_domain = app_test.copy()

    app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']
    app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']
    app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']
    app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']

    app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']
    app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']
    app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']
    app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']

    from sklearn.preprocessing import MinMaxScaler, Imputer

    # Drop the target from the training data
    if 'TARGET' in app_train:
        train = app_train.drop(columns=['TARGET'])
    else:
        train = app_train.copy()

    # Feature names
    features = list(train.columns)

    # Copy of the testing data
    test = app_test.copy()

    # Median imputation of missing values
    imputer = Imputer(strategy='median')

    # Scale each feature to 0-1
    scaler = MinMaxScaler(feature_range=(0, 1))

    # Fit on the training data
    imputer.fit(train)

    # Transform both training and testing data
    train = imputer.transform(train)
    test = imputer.transform(test)

    # Repeat with the scaler
    scaler.fit(train)
    train = scaler.transform(train)
    test = scaler.transform(test)

    print('Training data shape: ', train.shape)
    print('Testing data shape: ', test.shape)

    from sklearn.linear_model import LogisticRegression

    # Make the model with the specified regularization parameter
    log_reg = LogisticRegression(C=0.0001)

    # Train on the training data
    log_reg.fit(train, train_labels)

    # Make predictions
    # Make sure to select the second column only
    log_reg_pred = log_reg.predict_proba(test)[:, 1]

    # Submission dataframe
    submit = app_test[['SK_ID_CURR']]
    submit['TARGET'] = log_reg_pred

    submit.head()

    from sklearn.ensemble import RandomForestClassifier

    # Make the random forest classifier
    random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)

    # Train on the training data
    random_forest.fit(train, train_labels)

    # Extract feature importances
    feature_importance_values = random_forest.feature_importances_
    feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})

    # Make predictions on the test data
    predictions = random_forest.predict_proba(test)[:, 1]

    # Score = 0.678
    # Make a submission dataframe
    submit = app_test[['SK_ID_CURR']]
    submit['TARGET'] = predictions
    submit.head()

    poly_features_names = list(app_train_poly.columns)

    # Impute the polynomial features
    imputer = Imputer(strategy='median')

    poly_features = imputer.fit_transform(app_train_poly)
    poly_features_test = imputer.transform(app_test_poly)

    # Scale the polynomial features
    scaler = MinMaxScaler(feature_range=(0, 1))

    poly_features = scaler.fit_transform(poly_features)
    poly_features_test = scaler.transform(poly_features_test)

    random_forest_poly = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)
    random_forest_poly.fit(poly_features, train_labels)

    # Make predictions on the test data
    predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]

    # Score = 0.678
    # Make a submission dataframe
    submit = app_test[['SK_ID_CURR']]
    submit['TARGET'] = predictions
    submit.head()

    app_train_domain = app_train_domain.drop(columns='TARGET')

    domain_features_names = list(app_train_domain.columns)

    # Impute the domainnomial features
    imputer = Imputer(strategy='median')

    domain_features = imputer.fit_transform(app_train_domain)
    domain_features_test = imputer.transform(app_test_domain)

    # Scale the domainnomial features
    scaler = MinMaxScaler(feature_range=(0, 1))

    domain_features = scaler.fit_transform(domain_features)
    domain_features_test = scaler.transform(domain_features_test)

    # Train on the training data
    random_forest_domain = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)
    random_forest_domain.fit(domain_features, train_labels)

    # Extract feature importances
    feature_importance_values_domain = random_forest_domain.feature_importances_
    feature_importances_domain = pd.DataFrame(
        {'feature': domain_features_names, 'importance': feature_importance_values_domain})

    # Make predictions on the test data
    predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]

    # Score = 0.679
    # Make a submission dataframe
    submit = app_test[['SK_ID_CURR']]
    submit['TARGET'] = predictions
    submit.head()

    def plot_feature_importances(df):
        """
        Plot importances returned by a model. This can work with any measure of
        feature importance provided that higher importance is better.

        Args:
            df (dataframe): feature importances. Must have the features in a column
            called `features` and the importances in a column called `importance

        Returns:
            shows a plot of the 15 most importance features

            df (dataframe): feature importances sorted by importance (highest to lowest)
            with a column for normalized importance
            """

        # Sort features according to importance
        df = df.sort_values('importance', ascending=False).reset_index()

        # Normalize the feature importances to add up to one
        df['importance_normalized'] = df['importance'] / df['importance'].sum()

        # Make a horizontal bar chart of feature importances
        plt.figure(figsize=(10, 6))
        ax = plt.subplot()

        # Need to reverse the index to plot most important on top
        ax.barh(list(reversed(list(df.index[:15]))),
                df['importance_normalized'].head(15),
                align='center', edgecolor='k')

        # Set the yticks and labels
        ax.set_yticks(list(reversed(list(df.index[:15]))))
        ax.set_yticklabels(df['feature'].head(15))

        # Plot labeling
        plt.xlabel('Normalized Importance');
        plt.title('Feature Importances')
        plt.show()

        return df

    # Show the feature importances for the default features
    feature_importances_sorted = plot_feature_importances(feature_importances)

    feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)

    import lightgbm as lgb

    def model(features, test_features, encoding='ohe', n_folds=5):

        """Train and test a light gradient boosting model using
        cross validation.

        Parameters
        --------
            features (pd.DataFrame):
                dataframe of training features to use
                for training a model. Must include the TARGET column.
            test_features (pd.DataFrame):
                dataframe of testing features to use
                for making predictions with the model.
            encoding (str, default = 'ohe'):
                method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding
                n_folds (int, default = 5): number of folds to use for cross validation

        Return
        --------
            submission (pd.DataFrame):
                dataframe with `SK_ID_CURR` and `TARGET` probabilities
                predicted by the model.
            feature_importances (pd.DataFrame):
                dataframe with the feature importances from the model.
            valid_metrics (pd.DataFrame):
                dataframe with training and validation metrics (ROC AUC) for each fold and overall.

        """

        # Extract the ids
        train_ids = features['SK_ID_CURR']
        test_ids = test_features['SK_ID_CURR']

        # Extract the labels for training
        labels = features['TARGET']

        # Remove the ids and target
        features = features.drop(columns=['SK_ID_CURR', 'TARGET'])
        test_features = test_features.drop(columns=['SK_ID_CURR'])

        # One Hot Encoding
        if encoding == 'ohe':
            features = pd.get_dummies(features)
            test_features = pd.get_dummies(test_features)

            # Align the dataframes by the columns
            features, test_features = features.align(test_features, join='inner', axis=1)

            # No categorical indices to record
            cat_indices = 'auto'

        # Integer label encoding
        elif encoding == 'le':

            # Create a label encoder
            label_encoder = LabelEncoder()

            # List for storing categorical indices
            cat_indices = []

            # Iterate through each column
            for i, col in enumerate(features):
                if features[col].dtype == 'object':
                    # Map the categorical features to integers
                    features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))
                    test_features[col] = label_encoder.transform(
                        np.array(test_features[col].astype(str)).reshape((-1,)))

                    # Record the categorical indices
                    cat_indices.append(i)

        # Catch error if label encoding scheme is not valid
        else:
            raise ValueError("Encoding must be either 'ohe' or 'le'")

        print('Training Data Shape: ', features.shape)
        print('Testing Data Shape: ', test_features.shape)

        # Extract feature names
        feature_names = list(features.columns)

        # Convert to np arrays
        # features = np.array(features)
        # test_features = np.array(test_features)

        # Create the model
        model = lgb.LGBMClassifier(n_estimators=10, objective='binary',
                                   class_weight='balanced', learning_rate=0.05,
                                   reg_alpha=0.1, reg_lambda=0.1,
                                   subsample=0.8, n_jobs=-1, random_state=50)

        # Train the model
        model.fit(features, labels, eval_metric='auc',
                  categorical_feature=cat_indices,
                  verbose=200)

        # Record the best iteration
        best_iteration = model.best_iteration_
        predictions = model.predict_proba(test_features, num_iteration=best_iteration)[:, 1]

        # Record the feature importances
        feature_importance_values = model.feature_importances_

        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})

        return feature_importances

    fi = model(app_train, app_test)
    fi_sorted = plot_feature_importances(fi)

    app_train_domain['TARGET'] = train_labels

    # Test the domain knowledge features
    fi_domain = model(app_train_domain, app_test_domain)
    fi_sorted = plot_feature_importances(fi_domain)

    # it's difficult to capture model training time
    # as many of the operations are fit_transform
    return 0
